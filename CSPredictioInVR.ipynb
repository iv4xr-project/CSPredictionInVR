{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oIAcB6Rkqjn3",
        "69zDFqfogbOd",
        "v5Z5wWUzqnzW",
        "ZN3qlAcfSar9",
        "a9TIuU62Re1a"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS Prediction in VR\n",
        "\n",
        "## Author: \n",
        "###Tiago Manuel Severino GonÃ§alves. IST Lisboa, Portugal. Computer Engineering student number 89547"
      ],
      "metadata": {
        "id": "7J9J_84yDmJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library imports"
      ],
      "metadata": {
        "id": "w2ND5nXvqHjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all the necessary libraries\n",
        "\n",
        "* [numpy](https://numpy.org/): we will use it to store the data in array format for visualization\n",
        "* [pandas](https://pandas.pydata.org/): fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "built on top of the Python programming language. \n",
        "* [sklearn](https://scikit-learn.org/): provides a [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) implementation that we will use for visualization\n",
        "* [matplotlib](https://matplotlib.org/): plotting library for visualization\n",
        "* [tensorflow](https://www.tensorflow.org/): the neural network library"
      ],
      "metadata": {
        "id": "XMeKHpU4CwhM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4fX08RayCcVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ca0299-2689-431a-f5e4-7c8cf44cc682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "tensorflow version 2.9.2\n",
            "sklearn version 1.0.2\n",
            "keras version 2.9.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, StratifiedShuffleSplit, RepeatedStratifiedKFold, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # Will later be deprecated and should be replaced with scikeras (https://github.com/adriangb/scikeras)\n",
        "\n",
        "import multiprocessing\n",
        "print(multiprocessing.cpu_count())\n",
        "\n",
        "rootFolder = \"/content/drive/MyDrive/Tese/\" #change this to better suit your needs\n",
        "\n",
        "print(\"tensorflow version\",tf.__version__)\n",
        "print(\"sklearn version\",sklearn.__version__)\n",
        "print(\"keras version\",tf.keras.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation\n",
        "\n",
        "#### In subsequent runs of the program, this step can be skipped during later testing, provided you have run it at least once as it saves both the labels and the features in a suitable format, such as csv or a readable numpy array"
      ],
      "metadata": {
        "id": "00PGM4trp-Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labels\n",
        "\n",
        "##### Now let's get all the valid experiment identifiers from FMS_tag.csv as well as the labels for each timestep of features"
      ],
      "metadata": {
        "id": "oIAcB6Rkqjn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(rootFolder+\"/CSV_Data/TAGS/FMS_Tag.csv\",\"r\") as fmsTags:\n",
        "  fmsTagsDict = {}\n",
        "\n",
        "  lines = fmsTags.readlines()\n",
        "\n",
        "  for line in lines:\n",
        "    labels = []\n",
        "    tokens = line.split(',')\n",
        "    for token in tokens[1:]:\n",
        "      labels.append(int(token))\n",
        "    fmsTagsDict[tokens[0]] = labels\n",
        "  \n",
        "keys = fmsTagsDict.keys()\n",
        "\n",
        "\"\"\" The following converts the dictionary into a digestible csv file, with every\n",
        "    empty entry being filled with an Nan\"\"\"\n",
        "fmsTagsDF = pd.DataFrame({key: pd.Series(value) for key, value in fmsTagsDict.items()})\n",
        "fmsTagsDF.to_csv(rootFolder+\"/CSV_Data/LABELS/labels.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "\"\"\" The following is how to get a list of the labels for a certain key (in this case, '0')\n",
        "    without all the NaN values. It also converts all the values to int which is what we want\"\"\"\n",
        "#temp = fmsTagsDF['0'].dropna().astype(int).tolist()\n",
        "\n",
        "\"\"\"\n",
        "# locate in the labelsDF a label for a given key (ex '0') and minute (ex 18)\n",
        "# print(int(labelsDF['0'].iloc[18]))\n",
        "\n",
        "# or use a dict from the labelsDF\n",
        "\n",
        "labelsDict = {}\n",
        "for columnName, columnData in labelsDF.iteritems():\n",
        "  labelsDict[columnName] = labelsDF[columnName].dropna().astype(int).tolist()\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gKBA00wNEveq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a68acd00-72fc-4372-85f4-351d930d9c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# locate in the labelsDF a label for a given key (ex '0') and minute (ex 18)\\n# print(int(labelsDF['0'].iloc[18]))\\n\\n# or use a dict from the labelsDF\\n\\nlabelsDict = {}\\nfor columnName, columnData in labelsDF.iteritems():\\n  labelsDict[columnName] = labelsDF[columnName].dropna().astype(int).tolist()\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video Processing\n",
        "\n",
        "##### Here we process the video to remove features for the different approaches of Machine Learning, tradional and deep learning."
      ],
      "metadata": {
        "id": "69zDFqfogbOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 #OpenCv package\n",
        "import os\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "from skimage.measure import shannon_entropy\n",
        "\n",
        "videosDirectory = rootFolder+\"/RECORDINGS/\"\n",
        "\n",
        "# Deep Learning features\n",
        "\n",
        "deepLearningList = []\n",
        "totalMinutes = 0\n",
        "\n",
        "for key in keys: # keys being the experiments that were considered valid\n",
        "    filename = key +'.mp4' \n",
        "    title = filename.split('.')[0]\n",
        "    filename = videosDirectory + '/' + filename\n",
        "    video = cv2.VideoCapture(filename)\n",
        "\n",
        "    frameCount = 0\n",
        "    frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "    duration = int(frames/fps)\n",
        "    timestep = 60\n",
        "    framesPerTimestep = fps * timestep\n",
        "        \n",
        "    print(filename,duration//60,frames)\n",
        "\n",
        "    width = int(video.get(3))\n",
        "    height = int(video.get(4))\n",
        "\n",
        "    # HSV initialization\n",
        "    hueList = []\n",
        "    satList = []\n",
        "    brightList = []\n",
        "\n",
        "    hue = {}\n",
        "    [hue.setdefault(i, 0) for i in range(32)] \n",
        "    sat = {}\n",
        "    [sat.setdefault(i, 0) for i in range(32)] \n",
        "    bright = {}\n",
        "    [bright.setdefault(i, 0) for i in range(32)] \n",
        "\n",
        "    # Motion Intensity Initialization\n",
        "\n",
        "    flowList = []\n",
        "    meanFlowList = []\n",
        "    stdDevFlowList = []\n",
        "    skewnessFlowList = []\n",
        "    kurtosisFlowList = []\n",
        "    maxFlowList = []\n",
        "    minFlowList = []\n",
        "    varianceFlowList = []\n",
        "\n",
        "    # Contrast Initialization\n",
        "\n",
        "    contrastList = []\n",
        "    meanContrastList = []\n",
        "    stdDevContrastList = []\n",
        "    skewnessContrastList = []\n",
        "    kurtosisContrastList = []\n",
        "    maxContrastList = []\n",
        "    minContrastList = []\n",
        "    varianceContrastList = []\n",
        "\n",
        "    # Entropy Initialization\n",
        "\n",
        "    entropyList = []\n",
        "    meanEntropyList = []\n",
        "    stdDevEntropyList = []\n",
        "    skewnessEntropyList = []\n",
        "    kurtosisEntropyList = []\n",
        "    maxEntropyList = []\n",
        "    minEntropyList = []\n",
        "    varianceEntropyList = []\n",
        "\n",
        "    while True:\n",
        "        elapsedSeconds = frameCount/fps\n",
        "        #print(\"frameCount:\",frameCount, \"  elapsedSeconds: \", elapsedSeconds)\n",
        "\n",
        "        ret, frameImg = video.read()\n",
        "        frameCount+=1\n",
        "\n",
        "        #if(elapsedSeconds > 120): break\n",
        "\n",
        "        if(elapsedSeconds%timestep == 0 and elapsedSeconds > 0):\n",
        "            print(\"     TIMESTEP: \", elapsedSeconds/timestep)\n",
        "\n",
        "            # HSV statistics\n",
        "            hueList.append(hue)\n",
        "            satList.append(sat)\n",
        "            brightList.append(bright)\n",
        "\n",
        "            # Motion Intensity statistics\n",
        "            meanFlowList.append(np.mean(flowList))\n",
        "            stdDevFlowList.append(np.std(flowList))\n",
        "            skewnessFlowList.append(skew(flowList))\n",
        "            kurtosisFlowList.append(kurtosis(flowList))\n",
        "            maxFlowList.append(max(flowList))\n",
        "            minFlowList.append(min(flowList))\n",
        "            varianceFlowList.append(np.var(flowList))\n",
        "\n",
        "            # Contrast statistics\n",
        "            meanContrastList.append(np.mean(contrastList))\n",
        "            stdDevContrastList.append(np.std(contrastList))\n",
        "            skewnessContrastList.append(skew(contrastList))\n",
        "            kurtosisContrastList.append(kurtosis(contrastList))\n",
        "            maxContrastList.append(max(contrastList))\n",
        "            minContrastList.append(min(contrastList))\n",
        "            varianceContrastList.append(np.var(contrastList))\n",
        "\n",
        "\n",
        "            # Entropy statistics\n",
        "            meanEntropyList.append(np.mean(entropyList))\n",
        "            stdDevEntropyList.append(np.std(entropyList))\n",
        "            skewnessEntropyList.append(skew(entropyList))\n",
        "            kurtosisEntropyList.append(kurtosis(entropyList))\n",
        "            maxEntropyList.append(max(entropyList))\n",
        "            minEntropyList.append(min(entropyList))\n",
        "            varianceEntropyList.append(np.var(entropyList))\n",
        "\n",
        "            # Deep Learning features\n",
        "\n",
        "            deepLearningDFTemp = pd.DataFrame([flowList,contrastList,entropyList])\n",
        "            deepLearningDFTemp = deepLearningDFTemp.transpose()\n",
        "            deepLearningDFTemp.columns=['Flow','Contrast','Entropy']\n",
        "\n",
        "            deepLearningList.append(deepLearningDFTemp.to_numpy())\n",
        "\n",
        "            deepLearningDFTemp = pd.DataFrame()\n",
        "\n",
        "            # Reset temporary variables\n",
        "\n",
        "            hue = {}\n",
        "            [hue.setdefault(i, 0) for i in range(32)] \n",
        "            sat = {}\n",
        "            [sat.setdefault(i, 0) for i in range(32)] \n",
        "            bright = {}\n",
        "            [bright.setdefault(i, 0) for i in range(32)]\n",
        "\n",
        "            flowList = []\n",
        "            contrastList = []\n",
        "            entropyList = []\n",
        "\n",
        "        if(elapsedSeconds >= duration): break\n",
        "\n",
        "        # Resize so that its less computationally intensive to process\n",
        "\n",
        "        frameImg = cv2.resize(frameImg, (128, 72))\n",
        "\n",
        "        # HSV Capture\n",
        "\n",
        "        hsv = cv2.cvtColor(frameImg, cv2.COLOR_BGR2HSV)\n",
        "        h,s,v = cv2.split(hsv)\n",
        "        for j in range(len(h)):\n",
        "            for k in range(len(h[j])):\n",
        "                hue[h[j][k]//8] += 1\n",
        "                sat[s[j][k]//8] += 1\n",
        "                bright[v[j][k]//8] += 1\n",
        "\n",
        "        # Motion Intensity Capture\n",
        "\n",
        "        if(elapsedSeconds > 0):\n",
        "            gray = cv2.cvtColor(frameImg, cv2.COLOR_BGR2GRAY)\n",
        "            flow = cv2.calcOpticalFlowFarneback(prevgray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            prevgray = gray\n",
        "\n",
        "            # We use the mean of the flow because why not and I don't think we should just use the entire flow for all timestep*fps frames\n",
        "            flowList.append(np.mean(flow))\n",
        "\n",
        "        else:\n",
        "            prevgray = cv2.cvtColor(frameImg, cv2.COLOR_BGR2GRAY)\n",
        "            flowList.append(0)\n",
        "\n",
        "        # Contrast & Entropy Capture\n",
        "        contrastList.append(prevgray.std()) #RMS contrast\n",
        "        entropyList.append(shannon_entropy(prevgray)) #Shannon Entropy\n",
        "\n",
        "        # Display\n",
        "        #cv2.imshow('frame', hsv)\n",
        "        #if(elapsedSeconds > 0):\n",
        "        #    cv2.imshow('flow', draw_flow(gray, flow))\n",
        "        #    cv2.imshow('flow HSV', draw_hsv(flow))\n",
        "\n",
        "        if cv2.waitKey(1) == ord(\"q\"):\n",
        "            break\n",
        "\n",
        "    # Create Data Frames\n",
        "\n",
        "    motionDataFrame = pd.DataFrame()\n",
        "    motionDataFrame['Mean'] = meanFlowList\n",
        "    motionDataFrame['Std'] = stdDevFlowList\n",
        "    motionDataFrame['Skweness'] = skewnessFlowList\n",
        "    motionDataFrame['Kurtosis'] = kurtosisFlowList\n",
        "    motionDataFrame['Max'] = maxFlowList\n",
        "    motionDataFrame['Min'] = minFlowList\n",
        "    motionDataFrame['Var'] = varianceFlowList\n",
        "\n",
        "    contrastDataFrame = pd.DataFrame()\n",
        "    contrastDataFrame['Mean'] = meanContrastList\n",
        "    contrastDataFrame['Std'] = stdDevContrastList\n",
        "    contrastDataFrame['Skweness'] = skewnessContrastList\n",
        "    contrastDataFrame['Kurtosis'] = kurtosisContrastList\n",
        "    contrastDataFrame['Max'] = maxContrastList\n",
        "    contrastDataFrame['Min'] = minContrastList\n",
        "    contrastDataFrame['Var'] = varianceContrastList\n",
        "\n",
        "    entropyDataFrame = pd.DataFrame()\n",
        "    entropyDataFrame['Mean'] = meanEntropyList\n",
        "    entropyDataFrame['Std'] = stdDevEntropyList\n",
        "    entropyDataFrame['Skweness'] = skewnessEntropyList\n",
        "    entropyDataFrame['Kurtosis'] = kurtosisEntropyList\n",
        "    entropyDataFrame['Max'] = maxEntropyList\n",
        "    entropyDataFrame['Min'] = minEntropyList\n",
        "    entropyDataFrame['Var'] = varianceEntropyList\n",
        "\n",
        "    hueDataFrame = pd.DataFrame.from_dict(hueList, orient='columns')\n",
        "\n",
        "    satDataFrame = pd.DataFrame.from_dict(satList, orient='columns')\n",
        "\n",
        "    brightDataFrame = pd.DataFrame.from_dict(brightList, orient='columns')\n",
        "\n",
        "\n",
        "    # Write to CSV\n",
        "    videoStatsDirectory = '/content/drive/MyDrive/Tese/CSV_Data/VIDEOS/'+title+'/'\n",
        "\n",
        "    motionDataFrame.to_csv(videoStatsDirectory+title+\"_motion.csv\")\n",
        "    contrastDataFrame.to_csv(videoStatsDirectory+title+\"_contrast.csv\")\n",
        "    entropyDataFrame.to_csv(videoStatsDirectory+title+\"_entropy.csv\")\n",
        "    hueDataFrame.to_csv(videoStatsDirectory+title+\"_hue.csv\")\n",
        "    satDataFrame.to_csv(videoStatsDirectory+title+\"_sat.csv\")\n",
        "    brightDataFrame.to_csv(videoStatsDirectory+title+\"_bright.csv\")\n",
        "\n",
        "    video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    print(\"ended at: \", elapsedSeconds,\"  Minutes: \", int(elapsedSeconds/60))\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "\n",
        "    totalMinutes += int(elapsedSeconds/60)\n",
        "\n",
        "deepLearningFeatures = np.array(deepLearningList)\n",
        "#print(deepLearningFeatures)\n",
        "print(deepLearningFeatures.shape)\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/Tese/CSV_Data/FEATURES/deepLearningFeatures\", deepLearningFeatures) #adds .npy file extension\n",
        "\n",
        "print(totalMinutes)\n",
        "\n",
        "#deepLearningFeatures = np.load(\"/content/drive/MyDrive/Tese/CSV_Data/FEATURES/deepLearningFeatures.npy\")"
      ],
      "metadata": {
        "id": "rxUpLJiVgzAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "449dcd76-4f57-4365-b68b-6bd895dd7f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Tese/RECORDINGS/0.mp4 29 3480.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "     TIMESTEP:  29.0\n",
            "ended at:  1740.0   Minutes:  29\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/1.mp4 20 2400.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "ended at:  1200.0   Minutes:  20\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/3.mp4 19 2280.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "ended at:  1140.0   Minutes:  19\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/7.mp4 18 2160.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "ended at:  1080.0   Minutes:  18\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/8.mp4 29 3480.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "     TIMESTEP:  29.0\n",
            "ended at:  1740.0   Minutes:  29\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/9.mp4 25 3000.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "ended at:  1500.0   Minutes:  25\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/10.mp4 28 3360.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "ended at:  1680.0   Minutes:  28\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/11.mp4 27 3240.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "ended at:  1620.0   Minutes:  27\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/12.mp4 26 3120.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "ended at:  1560.0   Minutes:  26\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/13.mp4 24 2880.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "ended at:  1440.0   Minutes:  24\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/14.mp4 26 3120.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "ended at:  1560.0   Minutes:  26\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/15.mp4 26 3120.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "ended at:  1560.0   Minutes:  26\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/19.mp4 34 4080.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "     TIMESTEP:  29.0\n",
            "     TIMESTEP:  30.0\n",
            "     TIMESTEP:  31.0\n",
            "     TIMESTEP:  32.0\n",
            "     TIMESTEP:  33.0\n",
            "     TIMESTEP:  34.0\n",
            "ended at:  2040.0   Minutes:  34\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/20.mp4 30 3600.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "     TIMESTEP:  29.0\n",
            "     TIMESTEP:  30.0\n",
            "ended at:  1800.0   Minutes:  30\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/21.mp4 40 4800.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "     TIMESTEP:  29.0\n",
            "     TIMESTEP:  30.0\n",
            "     TIMESTEP:  31.0\n",
            "     TIMESTEP:  32.0\n",
            "     TIMESTEP:  33.0\n",
            "     TIMESTEP:  34.0\n",
            "     TIMESTEP:  35.0\n",
            "     TIMESTEP:  36.0\n",
            "     TIMESTEP:  37.0\n",
            "     TIMESTEP:  38.0\n",
            "     TIMESTEP:  39.0\n",
            "     TIMESTEP:  40.0\n",
            "ended at:  2400.0   Minutes:  40\n",
            "----------------------------------------------------------------\n",
            "(401, 120, 3)\n",
            "401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features\n",
        "\n",
        "##### Now that every valid experiment and their labels are in an easily digestible format, we can start building the features. This will be done by merging every one of the pre-processed csv files contained in the CSV_Data folder into one single digestible format"
      ],
      "metadata": {
        "id": "v5Z5wWUzqnzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSSQ and SSQ stats\n",
        "statsMSSQ_DF = pd.read_csv(rootFolder+\"/CSV_Data/STATS/PROCESSED_MSSQ.csv\")\n",
        "#statsSSQ_DF = pd.read_csv(rootFolder+\"/CSV_Data/STATS/PROCESSED_SSQ.csv\")\n",
        "\n",
        "# Create a DataFrame to hold the features of all experiments \n",
        "allFeaturesDF = pd.DataFrame()\n",
        "\n",
        "labels = pd.read_csv(rootFolder+\"/CSV_Data/LABELS/labels.csv\")\n",
        "keys = labels.columns\n",
        "\n",
        "for key in keys:\n",
        "\n",
        "  # Create a DataFrame to hold the features of the experiment\n",
        "  featuresDF = pd.DataFrame()\n",
        "\n",
        "  # Adding Inputs features (first modifying the 'Minute' column as they start from 1)\n",
        "  inputsDF = pd.read_csv(rootFolder+\"/CSV_Data/INPUTS/INPUTS-TRAD/\"+key+\".csv\")\n",
        "\n",
        "  duration = inputsDF['Minute'].values[-1]\n",
        "  featuresDF['Minute'] = pd.Series(range(duration)) # Add Minute column, starting on 0\n",
        "  featuresDF.insert(0,'Label',fmsTagsDict[key])\n",
        "  featuresDF.insert(0,'Id',key) # Add Id column to the start\n",
        "\n",
        "  inputsDF = inputsDF.drop(columns=['Minute'])\n",
        "  featuresDF = pd.concat([featuresDF,inputsDF],axis=1)\n",
        "\n",
        "  # Adding Video features (first removing an unnecessary 'Unnamed 0:' column)\n",
        "  hueDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_hue.csv\").iloc[: , 1:]\n",
        "  satDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_sat.csv\").iloc[: , 1:]\n",
        "  brightDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_bright.csv\").iloc[: , 1:]\n",
        "\n",
        "  \"\"\" The less error prone way to do this would be to do 3 seperate loops for each\n",
        "      color dataframe's columns since they could be different. But they aren't. \n",
        "      So I just use the same columns of hueDF for all 3 of them\"\"\"\n",
        "  for columnName, columnData in hueDF.iteritems():\n",
        "    hueDF.rename(columns = {columnName:\"hue_\"+columnName}, inplace = True)\n",
        "    satDF.rename(columns = {columnName:\"sat_\"+columnName}, inplace = True)\n",
        "    brightDF.rename(columns = {columnName:\"bright_\"+columnName}, inplace = True)\n",
        "\n",
        "  featuresDF = pd.concat([featuresDF,hueDF],axis=1)\n",
        "  featuresDF = pd.concat([featuresDF,satDF],axis=1)\n",
        "  featuresDF = pd.concat([featuresDF,brightDF],axis=1)\n",
        "\n",
        "  motionDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_motion.csv\").iloc[: , 1:]\n",
        "  contrastDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_contrast.csv\").iloc[: , 1:]\n",
        "  entropyDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_entropy.csv\").iloc[: , 1:]\n",
        "\n",
        "  \"\"\" Same as before, I know they have the same columns, so I can just do this\"\"\"\n",
        "  for columnName, columnData in motionDF.iteritems():\n",
        "    motionDF.rename(columns = {columnName:\"motion_\"+columnName}, inplace = True)\n",
        "    contrastDF.rename(columns = {columnName:\"contrast_\"+columnName}, inplace = True)\n",
        "    entropyDF.rename(columns = {columnName:\"entropy_\"+columnName}, inplace = True)\n",
        "\n",
        "  featuresDF = pd.concat([featuresDF,motionDF],axis=1)\n",
        "  featuresDF = pd.concat([featuresDF,contrastDF],axis=1)\n",
        "  featuresDF = pd.concat([featuresDF,entropyDF],axis=1)\n",
        "\n",
        "  # Adding Stats features\n",
        "  mssqRawScore = statsMSSQ_DF[statsMSSQ_DF[\"Id\"] == int(key)][\"MSSQ Raw score\"].iloc[0]\n",
        "  #ssqTotalScore = statsSSQ_DF[statsSSQ_DF[\"Id\"] == int(key)][\"TotalScore\"].iloc[0]\n",
        "\n",
        "  featuresDF[\"MSSQ\"] = mssqRawScore\n",
        "  #featuresDF[\"SSQ\"] = ssqTotalScore\n",
        "\n",
        "  filename = rootFolder+\"/CSV_Data/FEATURES/\" + key + \".csv\"\n",
        "  print(filename)\n",
        "  print(featuresDF.shape)\n",
        "  featuresDF.to_csv(filename, encoding='utf-8', index=False)\n",
        "\n",
        "  allFeaturesDF = pd.concat([allFeaturesDF, featuresDF])\n",
        "\n",
        "datasetDF = allFeaturesDF.drop(columns=['Minute','Id'])\n",
        "allFeaturesDF = allFeaturesDF.drop(columns=['Label'])\n",
        "\n",
        "filename = rootFolder+\"/CSV_Data/FEATURES/allFeatures.csv\"\n",
        "print(filename)\n",
        "print(allFeaturesDF.shape)\n",
        "allFeaturesDF.to_csv(filename, encoding='utf-8', index=False)\n",
        "\n",
        "filename = rootFolder+\"/CSV_Data/FEATURES/dataset.csv\"\n",
        "print(filename)\n",
        "print(datasetDF.shape)\n",
        "datasetDF.to_csv(filename, encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "K2sqqoEjo2Mk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461b2690-7dfd-4bcf-c0a2-25aa15c236c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/0.csv\n",
            "(29, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/1.csv\n",
            "(20, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/3.csv\n",
            "(19, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/7.csv\n",
            "(18, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/8.csv\n",
            "(29, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/9.csv\n",
            "(25, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/10.csv\n",
            "(28, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/11.csv\n",
            "(27, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/12.csv\n",
            "(26, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/13.csv\n",
            "(24, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/14.csv\n",
            "(26, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/15.csv\n",
            "(26, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/19.csv\n",
            "(34, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/20.csv\n",
            "(30, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/21.csv\n",
            "(40, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/allFeatures.csv\n",
            "(401, 126)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/dataset.csv\n",
            "(401, 125)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the video features we generated before\n",
        "videoDeepLearningFeatures = np.load(\"/content/drive/MyDrive/Tese/CSV_Data/FEATURES/videoDeepLearningFeatures.npy\")\n",
        "\n",
        "# Create an array to hold the input features of all experiments \n",
        "allInputs = []\n",
        "\n",
        "labels = pd.read_csv(\"/content/drive/MyDrive/Tese/CSV_Data/LABELS/labels.csv\")\n",
        "keys = labels.columns\n",
        "\n",
        "for key in keys:\n",
        "  # Getting input features dataframe for each valid experiment\n",
        "  inputsDF = pd.read_csv(\"/content/drive/MyDrive/Tese/CSV_Data/INPUTS/INPUTS-DEEP/\"+key+\".csv\")\n",
        "\n",
        "  inputsDF = inputsDF.drop(columns=['ElapsedSeconds'])\n",
        "\n",
        "  # iterate over each 120 input instances for this input data\n",
        "  inputs = []\n",
        "  for i in range(0,len(inputsDF)-1,120):\n",
        "    input = inputsDF.iloc[i:i+120].to_numpy()\n",
        "    inputs.append(input)\n",
        "\n",
        "  # Stack them all so we have a (<number_of_minutes>,120,7)\n",
        "  inputs = np.stack(inputs,axis=0)\n",
        "\n",
        "  allInputs.append(inputs)\n",
        "\n",
        "# Join all the input features\n",
        "allInputs = np.concatenate(allInputs,axis=0)\n",
        "\n",
        "# Join all the deep learning features\n",
        "deepFeatures = np.concatenate((allInputs,videoDeepLearningFeatures),axis=2)\n",
        "\n",
        "print(deepFeatures.shape)\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/Tese/CSV_Data/FEATURES/deepLearningFeatures\", deepFeatures) #adds .npy file extension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KF-cz8erhsh",
        "outputId": "027b65aa-09d4-4c48-967c-c34f26aa8ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(401, 120, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning\n",
        "\n",
        "#### Actually using the data we obtained"
      ],
      "metadata": {
        "id": "y7jUKJYCHfRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features and Labels Preperation\n",
        "\n"
      ],
      "metadata": {
        "id": "N1KHbAcQhPbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Learning Features and Labels"
      ],
      "metadata": {
        "id": "lMrxyxgufpbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Features and the Labels\n",
        "\n",
        "datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/TradFeaturesAndLabels.csv\")\n",
        "\n",
        "deep_y = datasetDF['Label'].to_numpy()\n",
        "deep_X = np.load(rootFolder+\"/CSV_Data/FEATURES/deepLearningFeatures.npy\")\n",
        "\n",
        "deep_X = tf.keras.utils.normalize(deep_X, axis=1)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Preparing the datasets:\n",
        "# 90% for training and 10% for testing\n",
        "deep_X_train, deep_X_test, deep_y_train, deep_y_test = train_test_split(deep_X, deep_y, test_size=0.10, shuffle=True, random_state=39)\n",
        "\n",
        "#sizes = datasetDF['Label'].value_counts(sort=1)\n",
        "#plt.pie(sizes,autopct='%1.1f%%') #60% vs 40%\n",
        "\n",
        "print(\"Classes\")\n",
        "print(datasetDF.iloc[:,0].value_counts()) # classes\n",
        "\n",
        "print(\"Features\")\n",
        "print(deep_y.shape) # 433 labels\n",
        "print(deep_X.shape) # (433,120,3) features\n",
        "\n",
        "# Show how many 0's and 1's are in the training and testing data\n",
        "print(\"0:\", np.count_nonzero(deep_y_train == 0), \"1:\", np.count_nonzero(deep_y_train == 1))\n",
        "print(\"0:\", np.count_nonzero(deep_y_test == 0), \"1:\", np.count_nonzero(deep_y_test == 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBkkDxmCfubY",
        "outputId": "3ae4c813-c00c-42a4-f112-cddd2dce3488"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes\n",
            "0    229\n",
            "1    172\n",
            "Name: Label, dtype: int64\n",
            "Features\n",
            "(401,)\n",
            "(401, 120, 10)\n",
            "0: 208 1: 152\n",
            "0: 21 1: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Traditional Learning Features and Labels"
      ],
      "metadata": {
        "id": "rDnWW7mBfu-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Features and the Labels\n",
        "\n",
        "datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/TradFeaturesAndLabels.csv\")\n",
        "\n",
        "trad_y = datasetDF['Label'].to_numpy()\n",
        "trad_X = datasetDF.drop(columns=['Label']).to_numpy()\n",
        "\n",
        "trad_X = tf.keras.utils.normalize(trad_X, axis=1)\n",
        "\n",
        "print(trad_X.shape)\n",
        "\n",
        "\n",
        "# Preparing the datasets:\n",
        "# 90% for training and 10% for testing\n",
        "trad_X_train, trad_X_test, trad_y_train, trad_y_test = train_test_split(trad_X, trad_y, test_size=0.10, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Classes\")\n",
        "print(datasetDF.iloc[:,0].value_counts()) # classes\n",
        "\n",
        "print(\"Features\")\n",
        "print(trad_y.shape) # 433 labels\n",
        "print(trad_X.shape) # 433*118 features"
      ],
      "metadata": {
        "id": "Q-Rx1RPmHmvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c83f64b-7b4a-46ba-ed18-786edce5bc33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(401, 124)\n",
            "Classes\n",
            "0    229\n",
            "1    172\n",
            "Name: Label, dtype: int64\n",
            "Features\n",
            "(401,)\n",
            "(401, 124)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Learning Models\n",
        "\n",
        "##### Here we will use a variety of machine learning models and compare results"
      ],
      "metadata": {
        "id": "lvL2k9T8F2yR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Learning Models\n",
        "\n",
        "###### Here we will use a variety of machine learning models and compare results"
      ],
      "metadata": {
        "id": "35HuWMWtex_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for CNN models\n",
        "\n",
        "def define_CNN( learning_rate=0.001, \n",
        "                convolutional_layers=1, \n",
        "                dropout=0.05, \n",
        "                kernels=16, \n",
        "                dense_layers=1, \n",
        "                dense_layer_kernels=16,\n",
        "                kernel_size=(3,3), \n",
        "                input_shape=(120,10,1), \n",
        "                pool_size=(2,2), \n",
        "                batch_normalization=False):\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  for i in range(convolutional_layers):\n",
        "    if(i == 0):\n",
        "      model.add(tf.keras.layers.Conv2D(kernels, kernel_size, activation='relu', strides=(1, 1), padding='same', input_shape=input_shape))\n",
        "    else:\n",
        "      model.add(tf.keras.layers.Conv2D(kernels, kernel_size, activation='relu', strides=(1, 1), padding='same'))\n",
        "    \n",
        "    if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=pool_size, padding='same'))\n",
        "\n",
        "    if(dropout > 0):\n",
        "      model.add(tf.keras.layers.Dropout(rate=dropout))\n",
        "  \n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  for i in range(dense_layers):\n",
        "    model.add(tf.keras.layers.Dense(dense_layer_kernels, activation='relu'))\n",
        "\n",
        "    if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[\"accuracy\"])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "euu6MtX3Sb5_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for LSTM-RNN models \n",
        "\n",
        "def define_LSTM( learning_rate=0.001, \n",
        "                 lstm_layers=1,\n",
        "                 units=10,\n",
        "                 dropout=0.05, \n",
        "                 dense_layers=1, \n",
        "                 dense_layer_kernels=16,\n",
        "                 input_shape=(120,10), \n",
        "                 batch_normalization=False):\n",
        "  \n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  for i in range(lstm_layers):\n",
        "    if(i == 0):\n",
        "      if(i == lstm_layers-1):\n",
        "        model.add(tf.keras.layers.LSTM(units, activation='relu', input_shape=input_shape))\n",
        "      else:\n",
        "        model.add(tf.keras.layers.LSTM(units, activation='relu', return_sequences=True, input_shape=input_shape))\n",
        "\n",
        "    elif(i == lstm_layers-1):\n",
        "      model.add(tf.keras.layers.LSTM(units, activation='relu'))\n",
        "    else:\n",
        "      model.add(tf.keras.layers.LSTM(units, activation='relu', return_sequences=True))\n",
        "    \n",
        "    if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if(dropout > 0):\n",
        "      model.add(tf.keras.layers.Dropout(rate=dropout))\n",
        "  \n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  for i in range(dense_layers):\n",
        "    model.add(tf.keras.layers.Dense(dense_layer_kernels, activation='relu'))\n",
        "\n",
        "    if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[\"accuracy\"])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "GKjSZxZPZBTp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### GridSearch the best parameters for CNN and LSTM"
      ],
      "metadata": {
        "id": "ZN3qlAcfSar9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### CNN "
      ],
      "metadata": {
        "id": "a9TIuU62Re1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search CNN\n",
        "\n",
        "scores = []\n",
        "totalScores = []\n",
        "\n",
        "sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10,random_state=42)\n",
        "batch_size = [None,4,8,16,32,64]\n",
        "epochs =  [7,9,10]\n",
        "\n",
        "# We'll see that if something performs better with the last entry of a given list,\n",
        "# we'll check if it perfoms better with the one after\n",
        "\n",
        "learning_rate = [0.001,0.0001]\n",
        "convolutional_layers=[1,2] # so for this one, if it perfoms better with 2, we'll try with 3 and so on\n",
        "dropout=[0.0,0.05,0.10]\n",
        "kernels=[8,16,32] # same for the other direction. if it performs better with 8, we'll try 4\n",
        "dense_layers=[1,2]\n",
        "dense_layer_kernels=[8,16,32]\n",
        "\n",
        "param_grid = dict(learning_rate=learning_rate,\n",
        "                  convolutional_layers=convolutional_layers,\n",
        "                  dropout=dropout,\n",
        "                  kernels=kernels,\n",
        "                  dense_layers=dense_layers,\n",
        "                  dense_layer_kernels=dense_layer_kernels)\n",
        "\n",
        "\n",
        "cnn_X = deep_X.reshape((deep_X.shape[0],deep_X.shape[1],deep_X.shape[2],1))\n",
        "\n",
        "for epoch in epochs:\n",
        "  for batch in batch_size:\n",
        "    model = KerasClassifier(build_fn = define_CNN,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch,\n",
        "                                verbose=1)\n",
        "    clf = GridSearchCV(estimator=model,param_grid=param_grid,cv=sssplit, return_train_score=False)\n",
        "    clf.fit(cnn_X,deep_y)\n",
        "    scores.append({\n",
        "        'model' : 'CNN',\n",
        "        'best_score' : clf.best_score_,\n",
        "        'best_params' : clf.best_params_,\n",
        "        'epochs' : epoch,\n",
        "        'batch_size' : batch\n",
        "    })\n",
        "\n",
        "    print(scores)\n",
        "    totalScores.append(scores)\n",
        "\n",
        "best_score = None\n",
        "maxScore = 0\n",
        "\n",
        "for Scores in totalScores:\n",
        "  for score in Scores:\n",
        "    if score['best_score'] >= maxScore:\n",
        "      best_score = score\n",
        "\n",
        "best_score"
      ],
      "metadata": {
        "id": "mGg8jhya_Bgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### LSTM"
      ],
      "metadata": {
        "id": "tQCLZk89Rg93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search LSTM\n",
        "\n",
        "scores = []\n",
        "totalScores = []\n",
        "\n",
        "sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10,random_state=42)\n",
        "batch_size = [None,4,8,16,32,64]\n",
        "epochs =  [5,10,20]\n",
        "\n",
        "# We'll see that if something performs better with the last entry of a given list,\n",
        "# we'll check if it perfoms better with the one after\n",
        "\n",
        "learning_rate = [0.001,0.0001]\n",
        "units = [10,25,50,75,100]\n",
        "lstm_layers=[1,2] # so for this one, if it perfoms better with 2, we'll try with 3 and so on\n",
        "dropout=[0.0,0.05,0.10]\n",
        "dense_layers=[1,2]\n",
        "dense_layer_kernels=[8,16,32]\n",
        "\n",
        "param_grid = dict(learning_rate=learning_rate,\n",
        "                  units=units,\n",
        "                  lstm_layers=lstm_layers,\n",
        "                  dropout=dropout,\n",
        "                  dense_layers=dense_layers,\n",
        "                  dense_layer_kernels=dense_layer_kernels)\n",
        "\n",
        "\n",
        "for epoch in epochs:\n",
        "  for batch in batch_size:\n",
        "    model = KerasClassifier(build_fn = define_LSTM,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch,\n",
        "                                verbose=1)\n",
        "    clf = GridSearchCV(estimator=model,param_grid=param_grid,cv=sssplit,return_train_score=False,verbose=1)\n",
        "    clf.fit(deep_X,deep_y)\n",
        "    scores.append({\n",
        "        'model' : 'CNN',\n",
        "        'best_score' : clf.best_score_,\n",
        "        'best_params' : clf.best_params_,\n",
        "        'epochs' : epoch,\n",
        "        'batch_size' : batch\n",
        "    })\n",
        "\n",
        "    print(scores)\n",
        "    totalScores.append(scores)\n",
        "\n",
        "best_score = None\n",
        "maxScore = 0\n",
        "\n",
        "for Scores in totalScores:\n",
        "  for score in Scores:\n",
        "    if score['best_score'] >= maxScore:\n",
        "      best_score = score\n",
        "\n",
        "best_score"
      ],
      "metadata": {
        "id": "StlWV2YgKu0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cross Validation "
      ],
      "metadata": {
        "id": "k_qz67QvVkNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### CNN "
      ],
      "metadata": {
        "id": "xiU3ZB01F-30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "From the GridSearch, we learned:\n",
        "\n",
        "36 models achieved a 'best_score': 0.6585365891456604\n",
        "\n",
        "'best_params': \n",
        "\n",
        "{\n",
        "  'learning_rate': ALWAYS 0.001\n",
        "  'convolutional_layers': ALWAYS 1,\n",
        "  'dropout': 0.05 OR 0.1\n",
        "  'kernels': 8 OR 16\n",
        "  'dense_layers': ALWAYS 1,\n",
        "  'dense_layer_kernels': ALWAYS 16,\n",
        "}, \n",
        "\n",
        "'epochs': ALWAYS 10\n",
        "'batch_size': 4 OR 64\n",
        "\"\"\"\n",
        "\n",
        "# set early stopping criteria\n",
        "pat = 5 #this is the number of epochs with no improvment after which the training will stop\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
        "\n",
        "# define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
        "#model_checkpoint = tf.keras.callbacks.ModelCheckpoint('fas_mnist_1.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "# reshape the feature dataset so it satisfies CNN input\n",
        "cnn_X_train = deep_X_train.reshape((deep_X_train.shape[0],deep_X_train.shape[1],deep_X_train.shape[2],1))\n",
        "cnn_X_test = deep_X_test.reshape((deep_X_test.shape[0],deep_X_test.shape[1],deep_X_test.shape[2],1))\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 4\n",
        "numLoops = 10\n",
        "verbose = 0\n",
        "\n",
        "total_results = []\n",
        "total_losses = []\n",
        "total_final_results = []\n",
        "total_final_losses = []\n",
        "\n",
        "# Run a StratifiedShuffleSplit 10 times on the dataset to approximate randomness\n",
        "\n",
        "for i in range(numLoops):\n",
        "  print(i)\n",
        "\n",
        "  sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10)\n",
        "\n",
        "  results = []\n",
        "  final_results = []\n",
        "  losses = []\n",
        "  final_losses = []\n",
        "  for train, test in sssplit.split(cnn_X_train, deep_y_train):\n",
        "    # Create the model with the best parameters\n",
        "    model = define_CNN(learning_rate=0.001, \n",
        "                     convolutional_layers=1, \n",
        "                     dropout=0.1, \n",
        "                     kernels=16, \n",
        "                     dense_layers=1, \n",
        "                     dense_layer_kernels=16,\n",
        "                     kernel_size=(3,3), \n",
        "                     input_shape=(120,10,1), \n",
        "                     pool_size=(2,2), \n",
        "                     batch_normalization=False)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(cnn_X_train[train], deep_y_train[train], epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stopping],  verbose=verbose)\n",
        "\n",
        "    # evaluate the model\n",
        "    scores = model.evaluate(cnn_X_train[test], deep_y_train[test], verbose=verbose)\n",
        "    #print(model.metrics_names[0], scores[0], model.metrics_names[1], scores[1])\n",
        "\n",
        "    results.append(scores[1])\n",
        "    losses.append(scores[0])\n",
        "\n",
        "    # validate the model\n",
        "    scores = model.evaluate(cnn_X_test, deep_y_test, verbose=verbose)\n",
        "    #print(model.metrics_names[0], scores[0], model.metrics_names[1], scores[1])\n",
        "\n",
        "    final_results.append(scores[1])\n",
        "    final_losses.append(scores[0])\n",
        "\n",
        "  #print(\"CNN-Mean - \", \"loss:\", np.mean(losses), \"accuracy:\", np.mean(results))\n",
        "  #print(\"CNN-STD - \", \"loss:\", np.std(losses), \"accuracy:\", np.std(results))\n",
        "  #print(\"CNN-Best - \", \"min-loss:\", np.amin(losses), \"max-accuracy:\", np.amax(results))\n",
        "\n",
        "  total_results.append(results)\n",
        "  total_losses.append(losses)\n",
        "\n",
        "  total_final_results.append(final_results)\n",
        "  total_final_losses.append(final_losses)\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in total_results:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"TRAINING ACCURACY\")\n",
        "print(\"CNN-Mean:\",np.mean(totals))\n",
        "print(\"CNN-STD:\",np.std(totals))\n",
        "print(\"CNN-Best:\",np.amax(totals))\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in total_losses:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"TRAINING LOSS\")\n",
        "print(\"CNN-Mean:\",np.mean(totals))\n",
        "print(\"CNN-STD:\",np.std(totals))\n",
        "print(\"CNN-Best:\",np.amin(totals))\n",
        "\n",
        "for results in total_final_results:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"TESTING ACCURACY\")\n",
        "print(\"CNN-Mean:\",np.mean(totals))\n",
        "print(\"CNN-STD:\",np.std(totals))\n",
        "print(\"CNN-Best:\",np.amax(totals))\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in total_final_losses:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"TESTING LOSS\")\n",
        "print(\"CNN-Mean:\",np.mean(totals))\n",
        "print(\"CNN-STD:\",np.std(totals))\n",
        "print(\"CNN-Best:\",np.amin(totals))"
      ],
      "metadata": {
        "id": "2dJ0gSyxdRmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a6ca61-2e2e-4beb-ce17-88e67feb5ab2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 10: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 6: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 7: early stopping\n",
            "Epoch 7: early stopping\n",
            "Epoch 6: early stopping\n",
            "Epoch 8: early stopping\n",
            "1\n",
            "Epoch 8: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 9: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 8: early stopping\n",
            "Epoch 6: early stopping\n",
            "2\n",
            "Epoch 10: early stopping\n",
            "Epoch 9: early stopping\n",
            "Epoch 9: early stopping\n",
            "Epoch 8: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 6: early stopping\n",
            "3\n",
            "Epoch 8: early stopping\n",
            "Epoch 8: early stopping\n",
            "Epoch 9: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 6: early stopping\n",
            "Epoch 6: early stopping\n",
            "4\n",
            "Epoch 8: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 7: early stopping\n",
            "Epoch 7: early stopping\n",
            "5\n",
            "Epoch 6: early stopping\n",
            "Epoch 7: early stopping\n",
            "Epoch 9: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 8: early stopping\n",
            "Epoch 7: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 7: early stopping\n",
            "Epoch 6: early stopping\n",
            "Epoch 10: early stopping\n",
            "6\n",
            "Epoch 10: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 8: early stopping\n",
            "Epoch 9: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 8: early stopping\n",
            "Epoch 9: early stopping\n",
            "Epoch 7: early stopping\n",
            "7\n",
            "Epoch 10: early stopping\n",
            "Epoch 6: early stopping\n",
            "Epoch 9: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 8: early stopping\n",
            "8\n",
            "Epoch 8: early stopping\n",
            "Epoch 7: early stopping\n",
            "Epoch 9: early stopping\n",
            "Epoch 7: early stopping\n",
            "Epoch 9: early stopping\n",
            "Epoch 8: early stopping\n",
            "Epoch 9: early stopping\n",
            "9\n",
            "Epoch 9: early stopping\n",
            "Epoch 6: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 6: early stopping\n",
            "Epoch 10: early stopping\n",
            "Epoch 9: early stopping\n",
            "CNN-ACCURACY\n",
            "CNN-Mean: 0.5969444453716278\n",
            "CNN-STD: 0.06695481333821544\n",
            "CNN-Best: 0.7777777910232544\n",
            "CNN-LOSS\n",
            "CNN-Mean: 0.7707300865650177\n",
            "CNN-STD: 0.13538565023502167\n",
            "CNN-Best: 0.5461486577987671\n",
            "FINAL CNN-ACCURACY\n",
            "CNN-Mean: 0.6878040680289268\n",
            "CNN-STD: 0.13067692963198604\n",
            "CNN-Best: 1.335808515548706\n",
            "FINAL CNN-LOSS\n",
            "CNN-Mean: 0.7556095570325851\n",
            "CNN-STD: 0.10480580317247869\n",
            "CNN-Best: 0.5419239401817322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### LSTM"
      ],
      "metadata": {
        "id": "C-77k5qLVWPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set early stopping criteria\n",
        "pat = 5 #this is the number of epochs with no improvment after which the training will stop\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
        "\n",
        "# define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
        "#model_checkpoint = tf.keras.callbacks.ModelCheckpoint('fas_mnist_1.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "numLoops = 10\n",
        "verbose = 0\n",
        "\n",
        "total_results = []\n",
        "total_losses = []\n",
        "total_final_results = []\n",
        "total_final_losses = []\n",
        "\n",
        "# Run a StratifiedShuffleSplit 10 times on the dataset to approximate randomness\n",
        "\n",
        "for i in range(numLoops):\n",
        "  print(i)\n",
        "\n",
        "  sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10)\n",
        "\n",
        "  results = []\n",
        "  final_results = []\n",
        "  losses = []\n",
        "  final_losses = []\n",
        "  for train, test in sssplit.split(deep_X_train, deep_y_train):\n",
        "    # Create the model with the best parameters\n",
        "    model = define_LSTM(  learning_rate=0.001, \n",
        "                          lstm_layers=1,\n",
        "                          units=32,\n",
        "                          dropout=0.05, \n",
        "                          dense_layers=1, \n",
        "                          dense_layer_kernels=16,\n",
        "                          input_shape=(120,10), \n",
        "                          batch_normalization=True)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(deep_X_train[train], deep_y_train[train], epochs=epochs, batch_size=batch_size, validation_split=0.1,  verbose=verbose)\n",
        "\n",
        "    # evaluate the model\n",
        "    scores = model.evaluate(deep_X_train[test], deep_y_train[test], verbose=verbose)\n",
        "    print(model.metrics_names[0], scores[0], model.metrics_names[1], scores[1])\n",
        "\n",
        "    results.append(scores[1])\n",
        "    losses.append(scores[0])\n",
        "\n",
        "    # validate the model\n",
        "    scores = model.evaluate(deep_X_test, deep_y_test, verbose=verbose)\n",
        "    #print(model.metrics_names[0], scores[0], model.metrics_names[1], scores[1])\n",
        "\n",
        "    final_results.append(scores[1])\n",
        "    final_losses.append(scores[0])\n",
        "\n",
        "  #print(\"LSTM-Mean - \", \"loss:\", np.mean(losses), \"accuracy:\", np.mean(results))\n",
        "  #print(\"LSTM-STD - \", \"loss:\", np.std(losses), \"accuracy:\", np.std(results))\n",
        "  #print(\"LSTM-Best - \", \"min-loss:\", np.amin(losses), \"max-accuracy:\", np.amax(results))\n",
        "\n",
        "  total_results.append(results)\n",
        "  total_losses.append(losses)\n",
        "\n",
        "  total_final_results.append(final_results)\n",
        "  total_final_losses.append(final_losses)\n",
        "\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in total_results:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"TRAINING LSTM-ACCURACY\")\n",
        "print(\"LSTM-Mean:\",np.mean(totals))\n",
        "print(\"LSTM-STD:\",np.std(totals))\n",
        "print(\"LSTM-Best:\",np.amax(totals))\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in total_losses:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"TRAINING LSTM-LOSS\")\n",
        "print(\"LSTM-Mean:\",np.mean(totals))\n",
        "print(\"LSTM-STD:\",np.std(totals))\n",
        "print(\"LSTM-Best:\",np.amin(totals))\n",
        "\n",
        "for results in total_final_results:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"TESTING LSTM-ACCURACY\")\n",
        "print(\"LSTM-Mean:\",np.mean(totals))\n",
        "print(\"LSTM-STD:\",np.std(totals))\n",
        "print(\"LSTM-Best:\",np.amax(totals))\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in total_final_losses:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"TESTING LSTM-LOSS\")\n",
        "print(\"LSTM-Mean:\",np.mean(totals))\n",
        "print(\"LSTM-STD:\",np.std(totals))\n",
        "print(\"LSTM-Best:\",np.amin(totals))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNxf7h4qQwBX",
        "outputId": "2253474f-8976-4ddc-e0f1-080d8cbd2fce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "loss 0.6845845580101013 accuracy 0.5833333134651184\n",
            "loss 0.6810071468353271 accuracy 0.5833333134651184\n",
            "loss 0.6822206377983093 accuracy 0.5833333134651184\n",
            "loss 0.6765050292015076 accuracy 0.5833333134651184\n",
            "loss 0.6908804774284363 accuracy 0.5833333134651184\n",
            "loss 0.6847965717315674 accuracy 0.5833333134651184\n",
            "loss 0.7156959176063538 accuracy 0.4166666567325592\n",
            "loss 0.6785590648651123 accuracy 0.5833333134651184\n",
            "loss 0.6765519380569458 accuracy 0.5833333134651184\n",
            "loss 0.6763119101524353 accuracy 0.5833333134651184\n",
            "1\n",
            "loss 0.6898219585418701 accuracy 0.5833333134651184\n",
            "loss 0.6772689819335938 accuracy 0.5833333134651184\n",
            "loss 0.6787360310554504 accuracy 0.5833333134651184\n",
            "loss 0.6791760325431824 accuracy 0.5833333134651184\n",
            "loss 0.6826456189155579 accuracy 0.5833333134651184\n",
            "loss 0.677897036075592 accuracy 0.5833333134651184\n",
            "loss 0.6855440139770508 accuracy 0.5833333134651184\n",
            "loss 0.680587112903595 accuracy 0.5833333134651184\n",
            "loss 0.6828782558441162 accuracy 0.5833333134651184\n",
            "loss 0.6800968647003174 accuracy 0.5833333134651184\n",
            "2\n",
            "loss 0.6817748546600342 accuracy 0.5833333134651184\n",
            "loss 0.6750731468200684 accuracy 0.5833333134651184\n",
            "loss 0.6834039092063904 accuracy 0.5833333134651184\n",
            "loss 0.6766705513000488 accuracy 0.5833333134651184\n",
            "loss 0.6833455562591553 accuracy 0.5833333134651184\n",
            "loss 0.691299319267273 accuracy 0.5833333134651184\n",
            "loss 0.6848763227462769 accuracy 0.5833333134651184\n",
            "loss 0.7168987989425659 accuracy 0.5833333134651184\n",
            "loss 0.6851655840873718 accuracy 0.5833333134651184\n",
            "loss 0.6750693917274475 accuracy 0.5833333134651184\n",
            "3\n",
            "loss 0.682180643081665 accuracy 0.5833333134651184\n",
            "loss 0.6803621649742126 accuracy 0.5833333134651184\n",
            "loss 0.687770664691925 accuracy 0.5833333134651184\n",
            "loss 0.6765185594558716 accuracy 0.5833333134651184\n",
            "loss 0.6745904088020325 accuracy 0.5833333134651184\n",
            "loss 0.6905970573425293 accuracy 0.5833333134651184\n",
            "loss 0.675875723361969 accuracy 0.5833333134651184\n",
            "loss 0.6855937242507935 accuracy 0.5833333134651184\n",
            "loss 0.6814348697662354 accuracy 0.5833333134651184\n",
            "loss 0.7076222896575928 accuracy 0.3888888955116272\n",
            "4\n",
            "loss 0.6920855045318604 accuracy 0.5833333134651184\n",
            "loss 0.6876323819160461 accuracy 0.5833333134651184\n",
            "loss 0.6979148983955383 accuracy 0.4166666567325592\n",
            "loss 0.6755449771881104 accuracy 0.5833333134651184\n",
            "loss 0.6840277314186096 accuracy 0.5833333134651184\n",
            "loss 0.7138780951499939 accuracy 0.4166666567325592\n",
            "loss 0.6770493388175964 accuracy 0.5833333134651184\n",
            "loss 0.6826047301292419 accuracy 0.5833333134651184\n",
            "loss 0.6802546381950378 accuracy 0.5833333134651184\n",
            "loss 0.6826500296592712 accuracy 0.5833333134651184\n",
            "5\n",
            "loss 0.6840245723724365 accuracy 0.5833333134651184\n",
            "loss 0.6873816847801208 accuracy 0.5833333134651184\n",
            "loss 0.6803884506225586 accuracy 0.5833333134651184\n",
            "loss 0.682202160358429 accuracy 0.5833333134651184\n",
            "loss 0.6774907112121582 accuracy 0.5833333134651184\n",
            "loss 0.6792975068092346 accuracy 0.5833333134651184\n",
            "loss 0.7028297781944275 accuracy 0.4444444477558136\n",
            "loss 0.6804338097572327 accuracy 0.5833333134651184\n",
            "loss 0.6762826442718506 accuracy 0.5833333134651184\n",
            "loss 0.6832755208015442 accuracy 0.5833333134651184\n",
            "6\n",
            "loss 0.6793844103813171 accuracy 0.5833333134651184\n",
            "loss 0.6785911321640015 accuracy 0.5833333134651184\n",
            "loss 0.6877479553222656 accuracy 0.5833333134651184\n",
            "loss 0.6828911304473877 accuracy 0.5833333134651184\n",
            "loss 0.6793323755264282 accuracy 0.5833333134651184\n",
            "loss 0.7180540561676025 accuracy 0.5833333134651184\n",
            "loss 0.6806761622428894 accuracy 0.5833333134651184\n",
            "loss 0.6805940866470337 accuracy 0.5833333134651184\n",
            "loss 0.6806706190109253 accuracy 0.5833333134651184\n",
            "loss 0.6818947196006775 accuracy 0.5833333134651184\n",
            "7\n",
            "loss 0.6786242127418518 accuracy 0.5833333134651184\n",
            "loss 0.6700291633605957 accuracy 0.5833333134651184\n",
            "loss 0.6953480243682861 accuracy 0.5\n",
            "loss 0.6801111698150635 accuracy 0.5833333134651184\n",
            "loss 0.6800458431243896 accuracy 0.5833333134651184\n",
            "loss 0.6989926695823669 accuracy 0.4444444477558136\n",
            "loss 0.6858713626861572 accuracy 0.5833333134651184\n",
            "loss 0.673217236995697 accuracy 0.5833333134651184\n",
            "loss 0.6871718168258667 accuracy 0.6111111044883728\n",
            "loss 0.6855526566505432 accuracy 0.5833333134651184\n",
            "8\n",
            "loss 0.6825811862945557 accuracy 0.5833333134651184\n",
            "loss 0.6828606128692627 accuracy 0.5833333134651184\n",
            "loss 0.682690441608429 accuracy 0.5833333134651184\n",
            "loss 0.6813124418258667 accuracy 0.5833333134651184\n",
            "loss 0.6787879467010498 accuracy 0.5833333134651184\n",
            "loss 0.6901828050613403 accuracy 0.5833333134651184\n",
            "loss 0.6806080341339111 accuracy 0.5833333134651184\n",
            "loss 0.6824122071266174 accuracy 0.5833333134651184\n",
            "loss 0.6794188022613525 accuracy 0.5833333134651184\n",
            "loss 0.6696506142616272 accuracy 0.5833333134651184\n",
            "9\n",
            "loss 0.6855869293212891 accuracy 0.5833333134651184\n",
            "loss 0.6952272653579712 accuracy 0.4722222089767456\n",
            "loss 0.6825810074806213 accuracy 0.5833333134651184\n",
            "loss 0.6761801242828369 accuracy 0.5833333134651184\n",
            "loss 0.6886598467826843 accuracy 0.5555555820465088\n",
            "loss 0.6882522106170654 accuracy 0.5555555820465088\n",
            "loss 0.6761062145233154 accuracy 0.5833333134651184\n",
            "loss 0.6826054453849792 accuracy 0.5833333134651184\n",
            "loss 0.6904701590538025 accuracy 0.5833333134651184\n",
            "loss 0.6774526238441467 accuracy 0.5833333134651184\n",
            "CNN-ACCURACY\n",
            "CNN-Mean: 0.5713888713717461\n",
            "CNN-STD: 0.04087299212067822\n",
            "CNN-Best: 0.6111111044883728\n",
            "CNN-LOSS\n",
            "CNN-Mean: 0.6840356558561325\n",
            "CNN-STD: 0.009066913945135709\n",
            "CNN-Best: 0.6696506142616272\n",
            "FINAL CNN-ACCURACY\n",
            "CNN-Mean: 0.5970178242027759\n",
            "CNN-STD: 0.08834101820927136\n",
            "CNN-Best: 0.7180540561676025\n",
            "FINAL CNN-LOSS\n",
            "CNN-Mean: 0.7052804440259933\n",
            "CNN-STD: 0.015844714917215407\n",
            "CNN-Best: 0.6877207159996033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Feature Reduction "
      ],
      "metadata": {
        "id": "cN-XHrt8Vo6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Traditional Machine Learning Models"
      ],
      "metadata": {
        "id": "kCHxIIGxfJtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### GridSearch the best parameters for SVM.SVC, RandomForest and Logistic Regression"
      ],
      "metadata": {
        "id": "4FNegvkLv_Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {\n",
        "    'svm' : {\n",
        "        'model' : svm.SVC(),\n",
        "        'params' : {\n",
        "            'C' : [0.5,1,5,10,25,50,100,500,1000],\n",
        "            'kernel' : ['rbf', 'linear', 'poly', 'sigmoid'],\n",
        "            'gamma' : [1, 0.1, 0.001, 0.0001, 'auto', 'scale'],\n",
        "            'degree' : [1, 2, 3, 4, 5, 6, 7]\n",
        "        }\n",
        "    },\n",
        "    'logistic_regression' : {\n",
        "        'model' : LogisticRegression(solver='liblinear', multi_class='auto'),\n",
        "        'params' : {\n",
        "            'C' : [0.5,1,5,10,25,50,100,500,1000],\n",
        "            'penalty' : ['l1', 'l2']\n",
        "        }\n",
        "    },\n",
        "    'random_forest' : {\n",
        "        'model' : RandomForestClassifier(),\n",
        "        'params' : {\n",
        "          'n_estimators' : [1,5,10,25,50,100,250,500,750,1000],\n",
        "          'criterion' : ['gini', 'entropy', 'log_loss'],\n",
        "          'max_features' : ['sqrt', 'log2', None]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "BtnL_OQUv-HS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "\n",
        "sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10,random_state=42)\n",
        "#sssplit=RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=42)\n",
        "\n",
        "for model_name, mp in model_params.items():\n",
        "  clf = GridSearchCV(mp['model'], mp['params'], cv=sssplit, return_train_score=False,verbose=1)\n",
        "  clf.fit(trad_X_train,trad_y_train)\n",
        "  scores.append({\n",
        "      'model' : model_name,\n",
        "      'best_estimator' : clf.best_estimator_,\n",
        "      'best_score' : clf.best_score_,\n",
        "      'best_params' : clf.best_params_\n",
        "  })\n",
        "\n",
        "scores"
      ],
      "metadata": {
        "id": "9zCQuiJixumL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e189fce-e32d-4c53-faab-33bb204bc18c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 1512 candidates, totalling 15120 fits\n",
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
            "Fitting 10 folds for each of 90 candidates, totalling 900 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "300 fits failed out of a total of 900.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "300 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
            "    trees = Parallel(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 1085, in __call__\n",
            "    if self.dispatch_one_batch(iterator):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
            "    self._dispatch(tasks)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 819, in _dispatch\n",
            "    job = self._backend.apply_async(batch, callback=cb)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
            "    result = ImmediateResult(func)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
            "    self.results = batch()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 288, in __call__\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
            "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/tree/_classes.py\", line 937, in fit\n",
            "    super().fit(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/tree/_classes.py\", line 352, in fit\n",
            "    criterion = CRITERIA_CLF[self.criterion](\n",
            "KeyError: 'log_loss'\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.61666667 0.64166667 0.67222222 0.69722222 0.7        0.71111111\n",
            " 0.71111111 0.70277778 0.71666667 0.71944444 0.59444444 0.61388889\n",
            " 0.7        0.70555556 0.70833333 0.69722222 0.70277778 0.70277778\n",
            " 0.71666667 0.71388889 0.65555556 0.66944444 0.65277778 0.73055556\n",
            " 0.71388889 0.68611111 0.71111111 0.70833333 0.71666667 0.71666667\n",
            " 0.6        0.65555556 0.7        0.7        0.70833333 0.69166667\n",
            " 0.725      0.71388889 0.70555556 0.71388889 0.6        0.625\n",
            " 0.675      0.66666667 0.68611111 0.725      0.71388889 0.70555556\n",
            " 0.70833333 0.70833333 0.6        0.63333333 0.675      0.7\n",
            " 0.71944444 0.70277778 0.71111111 0.71666667 0.71388889 0.70555556\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'model': 'svm',\n",
              "  'best_estimator': SVC(C=0.5, degree=2, gamma=1, kernel='poly'),\n",
              "  'best_score': 0.7055555555555555,\n",
              "  'best_params': {'C': 0.5, 'degree': 2, 'gamma': 1, 'kernel': 'poly'}},\n",
              " {'model': 'logistic_regression',\n",
              "  'best_estimator': LogisticRegression(C=25, solver='liblinear'),\n",
              "  'best_score': 0.7027777777777777,\n",
              "  'best_params': {'C': 25, 'penalty': 'l2'}},\n",
              " {'model': 'random_forest',\n",
              "  'best_estimator': RandomForestClassifier(max_features=None, n_estimators=25),\n",
              "  'best_score': 0.7305555555555554,\n",
              "  'best_params': {'criterion': 'gini',\n",
              "   'max_features': None,\n",
              "   'n_estimators': 25}}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cross Validation\n",
        "\n",
        "###### Having discovered the best parameters for each model, now we perform cross-validation, using StratifiedShuffleSplit, to train and test our models. The results of this process will tell us which model achieves the best accuracy."
      ],
      "metadata": {
        "id": "N_-3Epmg14Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVCResults = []\n",
        "LogisticRegressionResults = []\n",
        "RandomForestsResults = []\n",
        "\n",
        "for i in range(10):\n",
        "  print(i)\n",
        "\n",
        "  #sssplit=RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=42)\n",
        "  sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10)\n",
        "\n",
        "  modelSVC = svm.SVC(C = scores[0]['best_params']['C'], gamma=scores[0]['best_params']['gamma'], kernel=scores[0]['best_params']['kernel'], degree=scores[0]['best_params']['degree'])\n",
        "  results=sklearn.model_selection.cross_val_score(modelSVC,trad_X_train,trad_y_train,cv=sssplit)\n",
        "  #print(\"SVC-Mean:\",np.mean(results))\n",
        "  #print(\"SVC-STD:\",np.std(results))\n",
        "  #print(\"SVC-Best:\",np.amax(results))\n",
        "  SVCResults.append(results)\n",
        "\n",
        "\n",
        "  modelLogisticRegression = LogisticRegression(solver='liblinear', multi_class='auto', C = scores[1]['best_params']['C'])\n",
        "  results=sklearn.model_selection.cross_val_score(modelLogisticRegression,trad_X_train,trad_y_train,cv=sssplit)\n",
        "  #print(\"LogisticRegression-Mean:\",np.mean(results))\n",
        "  #print(\"LogisticRegression-STD:\",np.std(results))\n",
        "  #print(\"LogisticRegression-Best:\",np.amax(results))\n",
        "  LogisticRegressionResults.append(results)\n",
        "\n",
        "  modelRandomForest = RandomForestClassifier(n_estimators=scores[2]['best_params']['n_estimators'])\n",
        "  results=sklearn.model_selection.cross_val_score(modelRandomForest,trad_X_train,trad_y_train,cv=sssplit)\n",
        "  #print(\"RandomForest-Mean:\",np.mean(results))\n",
        "  #print(\"RandomForest-STD:\",np.std(results))\n",
        "  #print(\"RandomForest-Best:\",np.amax(results))\n",
        "  RandomForestsResults.append(results)\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in SVCResults:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"SVC-Mean:\",np.mean(totals))\n",
        "print(\"SVC-STD:\",np.std(totals))\n",
        "print(\"SVC-Best:\",np.amax(totals))\n",
        "\n",
        "print()\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in LogisticRegressionResults:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"LogisticRegression-Mean:\",np.mean(totals))\n",
        "print(\"LogisticRegression-STD:\",np.std(totals))\n",
        "print(\"LogisticRegression-Best:\",np.amax(totals))\n",
        "\n",
        "print()\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in RandomForestsResults:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"RandomForests-Mean:\",np.mean(totals))\n",
        "print(\"RandomForests-STD:\",np.std(totals))\n",
        "print(\"RandomForests-Best:\",np.amax(totals))\n",
        "\n",
        "print()\n",
        "\n",
        "# Show how many times a certain model beat the others\n",
        "\n",
        "wins = {'SVC' : 0, 'LR' : 0, 'RF' : 0, 'TIE' : 0}\n",
        "\n",
        "for i in range(len(SVCResults)):\n",
        "  for j in range(len(SVCResults[0])):\n",
        "    if SVCResults[i][j] >= LogisticRegressionResults[i][j] and SVCResults[i][j] >= RandomForestsResults[i][j]:\n",
        "      wins['SVC']+=1\n",
        "\n",
        "    if LogisticRegressionResults[i][j] >= SVCResults[i][j] and LogisticRegressionResults[i][j] >= RandomForestsResults[i][j]:\n",
        "      wins['LR']+=1\n",
        "\n",
        "    if RandomForestsResults[i][j] >= LogisticRegressionResults[i][j] and RandomForestsResults[i][j] >= SVCResults[i][j]:\n",
        "      wins['RF']+=1\n",
        "    else:\n",
        "      wins['TIE']+=1\n",
        "print(\"single:\",wins)\n",
        "\n",
        "# Show how many times a certain model beat the others but for the means\n",
        "\n",
        "wins = {'SVC' : 0, 'LR' : 0, 'RF' : 0, 'TIE' : 0}\n",
        "\n",
        "for i in range(len(SVCResults)):\n",
        "  if np.mean(SVCResults[i]) >= np.mean(LogisticRegressionResults[i]) and np.mean(SVCResults[i]) >= np.mean(RandomForestsResults[i]):\n",
        "    wins['SVC']+=1\n",
        "\n",
        "  if np.mean(LogisticRegressionResults[i]) >= np.mean(SVCResults[i]) and np.mean(LogisticRegressionResults[i]) >= np.mean(RandomForestsResults[i]):\n",
        "    wins['LR']+=1\n",
        "\n",
        "  if np.mean(RandomForestsResults[i]) >= np.mean(LogisticRegressionResults[i]) and np.mean(RandomForestsResults[i]) >= np.mean(SVCResults[i]):\n",
        "    wins['RF']+=1\n",
        "\n",
        "  else:\n",
        "    wins['TIE']+=1\n",
        "print(\"mean:\",wins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1JZEfYjdXGP",
        "outputId": "00660b01-727d-4bbd-94c7-b4e73250bcff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "SVC-Mean: 0.7105555555555557\n",
            "SVC-STD: 0.06445162795212402\n",
            "SVC-Best: 0.8333333333333334\n",
            "\n",
            "LogisticRegression-Mean: 0.7205555555555554\n",
            "LogisticRegression-STD: 0.06836042823262703\n",
            "LogisticRegression-Best: 0.8611111111111112\n",
            "\n",
            "RandomForests-Mean: 0.7022222222222224\n",
            "RandomForests-STD: 0.06359594676112972\n",
            "RandomForests-Best: 0.8611111111111112\n",
            "\n",
            "single: {'SVC': 43, 'LR': 43, 'RF': 32, 'TIE': 68}\n",
            "mean: {'SVC': 4, 'LR': 4, 'RF': 2, 'TIE': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Features reduction\n",
        "\n",
        "###### First, we must discover which features have less impact, for each model's best estimator. \n",
        "\n",
        "######Then, remove them from the dataset and check the results of cross-validation again. \n",
        "\n",
        "######This process will ideally report the best results possible."
      ],
      "metadata": {
        "id": "kjV78LpA12IJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next code block actually serves two functions:\n",
        "\n",
        "\n",
        "*   Discover features impact on a model\n",
        "*   As well as discover the final testing results\n",
        "\n"
      ],
      "metadata": {
        "id": "bOGGGQxuSsCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVC\n",
        "# If the best kernel parameter for SVC was non-linear, then we cannot investigate feature importance\n",
        "# This is because non-linear kernels map the dataset into a space of higher dimension\n",
        "\n",
        "if(scores[0]['best_params']['kernel'] == 'linear'):\n",
        "  model = scores[0]['best_estimator']\n",
        "  prediction_test = model.predict(trad_X_test)\n",
        "  print(\"SVC-Accuracy:\",sklearn.metrics.accuracy_score(trad_y_test,prediction_test))\n",
        "\n",
        "  weightsSCVDF = pd.DataFrame(pd.Series(modelSVC.coef_[0], index=datasetDF.drop(columns=['Label']).columns).sort_values(ascending=False)).rename(columns = {0:'feature'})\n",
        "  weightsSCVDF.to_csv(rootFolder+\"/CSV_Data/FEATURES/weightsSCV.csv\", encoding='utf-8')\n",
        "\n",
        "  #sklearn.metrics.ConfusionMatrixDisplay.from_predictions(trad_y_test, prediction_test) #confusion matrix\n",
        "\n",
        "# Logistic Regression\n",
        "model = scores[1]['best_estimator']\n",
        "#model.fit(trad_X_train, trad_y_train)\n",
        "prediction_test = model.predict(trad_X_test)\n",
        "print(\"LogisticRegression-Accuracy:\",sklearn.metrics.accuracy_score(trad_y_test,prediction_test))\n",
        "\n",
        "weightsLogisticRegressionDF = pd.DataFrame(pd.Series(model.coef_[0], index=datasetDF.drop(columns=['Label']).columns).sort_values(ascending=False)).rename(columns = {0:'feature'})\n",
        "weightsLogisticRegressionDF.to_csv(rootFolder+\"/CSV_Data/FEATURES/weightsLogisticRegression.csv\", encoding='utf-8')\n",
        "\n",
        "#sklearn.metrics.ConfusionMatrixDisplay(\"Logistic Regression\").from_predictions(trad_y_test, prediction_test) #confusion matrix\n",
        "\n",
        "# Random Forest\n",
        "model = scores[2]['best_estimator']\n",
        "prediction_test = model.predict(trad_X_test)\n",
        "print(\"RandomForest-Accuracy:\",sklearn.metrics.accuracy_score(trad_y_test,prediction_test))\n",
        "\n",
        "weightsRandomForestDF = pd.Series(model.feature_importances_, index=datasetDF.drop(columns=['Label']).columns).sort_values(ascending=False).to_frame().rename(columns = {0:'feature'})\n",
        "weightsRandomForestDF.to_csv(rootFolder+\"/CSV_Data/FEATURES/weightsRandomForest.csv\", encoding='utf-8')\n",
        "\n",
        "#sklearn.metrics.ConfusionMatrixDisplay(\"Random Forest\").from_predictions(trad_y_test, prediction_test) #confusion matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoYnkAH2w3s3",
        "outputId": "3bc88384-489d-4d83-8cf7-3d97a9439a4b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression-Accuracy: 0.6829268292682927\n",
            "RandomForest-Accuracy: 0.6097560975609756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We're going to try different thresholds for removing features\n",
        "\n",
        "SVCThresholds = [0.25,0.5,1.0,1.5,2.0,2.5,3.0]\n",
        "LRThresholds = [0.25,0.5,1.0,1.5,2.0,2.5,3.0]\n",
        "RFThresholds = [0.003,0.004,0.005,0.006,0.007,0.008,0.009]\n",
        "\n",
        "for k in range(len(SVCThresholds)):\n",
        "  print(\"iteration k:\",k)\n",
        "\n",
        "  ######################################################\n",
        "  # Remove the unnecessary features from the dataset\n",
        "  ######################################################\n",
        "\n",
        "  # SVC\n",
        "  if(scores[0]['best_params']['kernel'] == 'linear'):\n",
        "    datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/dataset.csv\")\n",
        "  \n",
        "    SVC_y = datasetDF['Label'].to_numpy()\n",
        "    SVC_X = datasetDF.drop(columns=['Label'])\n",
        "  \n",
        "    # Those that have feature importance below 1 and above -1 are probably safe to ignore\n",
        "    #weightsSCVDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/weightsSCV.csv\")\n",
        "    featuresToRemove = weightsSCVDF.loc[(weightsSCVDF['feature'] >= -SVCThresholds[k]) & (weightsSCVDF['feature'] <= SVCThresholds[k])].index.values.tolist()\n",
        "    SVC_X = SVC_X.drop(columns=featuresToRemove)\n",
        "  \n",
        "    SVC_X = tf.keras.utils.normalize(SVC_X, axis=1)\n",
        "\n",
        "    # 90% for training and 10% for testing\n",
        "    SVC_X_train, SVC_X_test, SVC_y_train, SVC_y_test = train_test_split(SVC_X, SVC_y, test_size=0.10, shuffle=True, random_state=42)\n",
        "  \n",
        "  \n",
        "  # LogisticRegression\n",
        "  datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/dataset.csv\")\n",
        "  \n",
        "  LR_y = datasetDF['Label'].to_numpy()\n",
        "  LR_X = datasetDF.drop(columns=['Label'])\n",
        "  \n",
        "  # Those that have feature importance below 1 and above -1 are probably safe to ignore\n",
        "  #weightsLogisticRegressionDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/weightsLogisticRegression.csv\")\n",
        "  featuresToRemove = weightsLogisticRegressionDF.loc[(weightsLogisticRegressionDF['feature'] >= -LRThresholds[k]) & (weightsLogisticRegressionDF['feature'] <= LRThresholds[k])].index.values.tolist()\n",
        "  LR_X = LR_X.drop(columns=featuresToRemove)\n",
        "  \n",
        "  LR_X = tf.keras.utils.normalize(LR_X, axis=1)\n",
        "\n",
        "  # 90% for training and 10% for testing\n",
        "  LR_X_train, LR_X_test, LR_y_train, LR_y_test = train_test_split(LR_X, LR_y, test_size=0.10, shuffle=True, random_state=42)\n",
        "    \n",
        "  # RandomForest\n",
        "  datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/dataset.csv\")\n",
        "  \n",
        "  RF_y = datasetDF['Label'].to_numpy()\n",
        "  RF_X = datasetDF.drop(columns=['Label'])\n",
        "  \n",
        "  # Those that have feature coeficients that are below 0.006 are probably safe to ignore\n",
        "  #weightsRandomForestDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/weightsRandomForest.csv\")\n",
        "  featuresToRemove = weightsRandomForestDF.loc[(weightsRandomForestDF['feature'] <= RFThresholds[k])].index.values.tolist()\n",
        "  RF_X = RF_X.drop(columns=featuresToRemove)\n",
        "  \n",
        "  RF_X = tf.keras.utils.normalize(RF_X, axis=1)\n",
        "  \n",
        "  # 90% for training and 10% for testing\n",
        "  RF_X_train, RF_X_test, RF_y_train, RF_y_test = train_test_split(RF_X, RF_y, test_size=0.10, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "  ######################################################\n",
        "  # And check the results of cross-validation again\n",
        "  ######################################################\n",
        "  SVCResults = []\n",
        "  LogisticRegressionResults = []\n",
        "  RandomForestsResults = []\n",
        "\n",
        "  for i in range(10):\n",
        "    print(i)\n",
        "\n",
        "    #sssplit=RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=42)\n",
        "    sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10)\n",
        "\n",
        "    if(scores[0]['best_params']['kernel'] == 'linear'):\n",
        "      modelSVC = svm.SVC(C = scores[0]['best_params']['C'], gamma=scores[0]['best_params']['gamma'], kernel=scores[0]['best_params']['kernel'], degree=scores[0]['best_params']['degree'])\n",
        "      results=sklearn.model_selection.cross_val_score(modelSVC,SVC_X_train,SVC_y_train,cv=sssplit)\n",
        "      #print(\"SVC-Mean:\",np.mean(results))\n",
        "      #print(\"SVC-STD:\",np.std(results))\n",
        "      #print(\"SVC-Best:\",np.amax(results))\n",
        "      SVCResults.append(results)\n",
        "\n",
        "\n",
        "    modelLogisticRegression = LogisticRegression(solver='liblinear', multi_class='auto', C = scores[1]['best_params']['C'])\n",
        "    results=sklearn.model_selection.cross_val_score(modelLogisticRegression,LR_X_train,LR_y_train,cv=sssplit)\n",
        "    #print(\"LogisticRegression-Mean:\",np.mean(results))\n",
        "    #print(\"LogisticRegression-STD:\",np.std(results))\n",
        "    #print(\"LogisticRegression-Best:\",np.amax(results))\n",
        "    LogisticRegressionResults.append(results)\n",
        "\n",
        "    modelRandomForest = RandomForestClassifier(n_estimators=scores[2]['best_params']['n_estimators'])\n",
        "    results=sklearn.model_selection.cross_val_score(modelRandomForest,RF_X_train,RF_y_train,cv=sssplit)\n",
        "    #print(\"RandomForest-Mean:\",np.mean(results))\n",
        "    #print(\"RandomForest-STD:\",np.std(results))\n",
        "    #print(\"RandomForest-Best:\",np.amax(results))\n",
        "    RandomForestsResults.append(results)\n",
        "\n",
        "\n",
        "\n",
        "  ######################################################\n",
        "  # Now check the scores\n",
        "  ######################################################\n",
        "  if(scores[0]['best_params']['kernel'] == 'linear'):\n",
        "    totals = []\n",
        "\n",
        "    for results in SVCResults:\n",
        "      for result in results:\n",
        "        totals.append(result)\n",
        "\n",
        "    print(\"SVC-Mean:\",np.mean(totals))\n",
        "    print(\"SVC-STD:\",np.std(totals))\n",
        "    print(\"SVC-Best:\",np.amax(totals))\n",
        "    print(\"SVC-Threshold\",SVCThresholds[k])\n",
        "\n",
        "    print()\n",
        "\n",
        "  totals = []\n",
        "\n",
        "  for results in LogisticRegressionResults:\n",
        "    for result in results:\n",
        "      totals.append(result)\n",
        "\n",
        "  print(\"LogisticRegression-Mean:\",np.mean(totals))\n",
        "  print(\"LogisticRegression-STD:\",np.std(totals))\n",
        "  print(\"LogisticRegression-Best:\",np.amax(totals))\n",
        "  print(\"LogisticRegression-Threshold\",LRThresholds[k])\n",
        "  \n",
        "\n",
        "  print()\n",
        "\n",
        "  totals = []\n",
        "\n",
        "  for results in RandomForestsResults:\n",
        "    for result in results:\n",
        "      totals.append(result)\n",
        "\n",
        "  print(\"RandomForests-Mean:\",np.mean(totals))\n",
        "  print(\"RandomForests-STD:\",np.std(totals))\n",
        "  print(\"RandomForests-Best:\",np.amax(totals))\n",
        "  print(\"RandomForests-Threshold\",RFThresholds[k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkeImJVf1ESa",
        "outputId": "d428d44d-bab0-4314-ac78-208bbeb043bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration k: 0\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.721388888888889\n",
            "LogisticRegression-STD: 0.071303450819476\n",
            "LogisticRegression-Best: 0.8888888888888888\n",
            "LogisticRegression-Threshold 0.25\n",
            "\n",
            "RandomForests-Mean: 0.6908333333333334\n",
            "RandomForests-STD: 0.07121682680204929\n",
            "RandomForests-Best: 0.8333333333333334\n",
            "RandomForests-Threshold 0.003\n",
            "iteration k: 1\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.7247222222222222\n",
            "LogisticRegression-STD: 0.06962144110637467\n",
            "LogisticRegression-Best: 0.8888888888888888\n",
            "LogisticRegression-Threshold 0.5\n",
            "\n",
            "RandomForests-Mean: 0.7172222222222223\n",
            "RandomForests-STD: 0.07268866469614532\n",
            "RandomForests-Best: 0.8611111111111112\n",
            "RandomForests-Threshold 0.004\n",
            "iteration k: 2\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.7205555555555557\n",
            "LogisticRegression-STD: 0.061716696422313096\n",
            "LogisticRegression-Best: 0.8333333333333334\n",
            "LogisticRegression-Threshold 1.0\n",
            "\n",
            "RandomForests-Mean: 0.708611111111111\n",
            "RandomForests-STD: 0.06484728022890722\n",
            "RandomForests-Best: 0.8611111111111112\n",
            "RandomForests-Threshold 0.005\n",
            "iteration k: 3\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.7252777777777777\n",
            "LogisticRegression-STD: 0.06222408231346678\n",
            "LogisticRegression-Best: 0.8888888888888888\n",
            "LogisticRegression-Threshold 1.5\n",
            "\n",
            "RandomForests-Mean: 0.7058333333333334\n",
            "RandomForests-STD: 0.06895326293320574\n",
            "RandomForests-Best: 0.8888888888888888\n",
            "RandomForests-Threshold 0.006\n",
            "iteration k: 4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.6997222222222222\n",
            "LogisticRegression-STD: 0.0746075121163917\n",
            "LogisticRegression-Best: 0.8611111111111112\n",
            "LogisticRegression-Threshold 2.0\n",
            "\n",
            "RandomForests-Mean: 0.711666666666667\n",
            "RandomForests-STD: 0.06936439831128374\n",
            "RandomForests-Best: 0.8611111111111112\n",
            "RandomForests-Threshold 0.007\n",
            "iteration k: 5\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.7072222222222223\n",
            "LogisticRegression-STD: 0.063879226322456\n",
            "LogisticRegression-Best: 0.8888888888888888\n",
            "LogisticRegression-Threshold 2.5\n",
            "\n",
            "RandomForests-Mean: 0.7069444444444446\n",
            "RandomForests-STD: 0.07204840194079411\n",
            "RandomForests-Best: 0.8611111111111112\n",
            "RandomForests-Threshold 0.008\n",
            "iteration k: 6\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.6602777777777779\n",
            "LogisticRegression-STD: 0.08154138160210585\n",
            "LogisticRegression-Best: 0.8611111111111112\n",
            "LogisticRegression-Threshold 3.0\n",
            "\n",
            "RandomForests-Mean: 0.7141666666666667\n",
            "RandomForests-STD: 0.07610249342940639\n",
            "RandomForests-Best: 0.8888888888888888\n",
            "RandomForests-Threshold 0.009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given the results from the code block above, we write the best thresholds for each model below\n",
        "\n",
        "svcBestThreshold = 0 # Never got a threshold for SVC models\n",
        "lrBestThreshold = 1.5\n",
        "rfBestThreshold = 0.009\n",
        "\n",
        "\n",
        "# SVC\n",
        "if(scores[0]['best_params']['kernel'] == 'linear'):\n",
        "  datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/dataset.csv\")\n",
        "  \n",
        "  SVC_y = datasetDF['Label'].to_numpy()\n",
        "  SVC_X = datasetDF.drop(columns=['Label'])\n",
        "  \n",
        "  # Those that have feature importance below 1 and above -1 are probably safe to ignore\n",
        "  #weightsSCVDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/weightsSCV.csv\")\n",
        "  featuresToRemove = weightsSCVDF.loc[(weightsSCVDF['feature'] >= -svcBestThreshold) & (weightsSCVDF['feature'] <= svcBestThreshold)].index.values.tolist()\n",
        "  SVC_X = SVC_X.drop(columns=featuresToRemove)\n",
        "  \n",
        "  SVC_X = tf.keras.utils.normalize(SVC_X, axis=1)\n",
        "\n",
        "  # 90% for training and 10% for testing\n",
        "  SVC_X_train, SVC_X_test, SVC_y_train, SVC_y_test = train_test_split(SVC_X, SVC_y, test_size=0.10, shuffle=True, random_state=42)\n",
        "  \n",
        "  \n",
        "# LogisticRegression\n",
        "datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/dataset.csv\")\n",
        "  \n",
        "LR_y = datasetDF['Label'].to_numpy()\n",
        "LR_X = datasetDF.drop(columns=['Label'])\n",
        "  \n",
        "# Those that have feature importance below 1 and above -1 are probably safe to ignore\n",
        "#weightsLogisticRegressionDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/weightsLogisticRegression.csv\")\n",
        "featuresToRemove = weightsLogisticRegressionDF.loc[(weightsLogisticRegressionDF['feature'] >= -lrBestThreshold) & (weightsLogisticRegressionDF['feature'] <= lrBestThreshold)].index.values.tolist()\n",
        "LR_X = LR_X.drop(columns=featuresToRemove)\n",
        "  \n",
        "LR_X = tf.keras.utils.normalize(LR_X, axis=1)\n",
        "\n",
        "# 90% for training and 10% for testing\n",
        "LR_X_train, LR_X_test, LR_y_train, LR_y_test = train_test_split(LR_X, LR_y, test_size=0.10, shuffle=True, random_state=42)\n",
        "    \n",
        "# RandomForest\n",
        "datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/dataset.csv\")\n",
        "  \n",
        "RF_y = datasetDF['Label'].to_numpy()\n",
        "RF_X = datasetDF.drop(columns=['Label'])\n",
        "  \n",
        "# Those that have feature coeficients that are below 0.006 are probably safe to ignore\n",
        "#weightsRandomForestDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/weightsRandomForest.csv\")\n",
        "featuresToRemove = weightsRandomForestDF.loc[(weightsRandomForestDF['feature'] <= rfBestThreshold)].index.values.tolist()\n",
        "RF_X = RF_X.drop(columns=featuresToRemove)\n",
        "  \n",
        "RF_X = tf.keras.utils.normalize(RF_X, axis=1)\n",
        "  \n",
        "# 90% for training and 10% for testing\n",
        "RF_X_train, RF_X_test, RF_y_train, RF_y_test = train_test_split(RF_X, RF_y, test_size=0.10, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# Training & Testing\n",
        "\n",
        "sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10)\n",
        "\n",
        "if(scores[0]['best_params']['kernel'] == 'linear'):\n",
        "  modelSVC = svm.SVC(C = scores[0]['best_params']['C'], gamma=scores[0]['best_params']['gamma'], kernel=scores[0]['best_params']['kernel'], degree=scores[0]['best_params']['degree'])\n",
        "  print(modelSVC.fit(SVC_X_train,SVC_y_train))\n",
        "  prediction_test = modelSVC.predict(SVC_X_test)\n",
        "  print(\"SVC-Accuracy:\",sklearn.metrics.accuracy_score(SVC_y_test,prediction_test))\n",
        "\n",
        "\n",
        "modelLogisticRegression = LogisticRegression(solver='liblinear', multi_class='auto', C = scores[1]['best_params']['C'])\n",
        "print(modelLogisticRegression.fit(LR_X_train,LR_y_train))\n",
        "prediction_test = modelLogisticRegression.predict(LR_X_test)\n",
        "print(\"LogisticRegression-Accuracy:\",sklearn.metrics.accuracy_score(LR_y_test,prediction_test))\n",
        "\n",
        "modelRandomForest = RandomForestClassifier(n_estimators=scores[2]['best_params']['n_estimators'])\n",
        "print(modelRandomForest.fit(RF_X_train,RF_y_train))\n",
        "prediction_test = modelRandomForest.predict(RF_X_test)\n",
        "print(\"RandomForest-Accuracy:\",sklearn.metrics.accuracy_score(RF_y_test,prediction_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJPPxDe_VS4L",
        "outputId": "1265f37c-94dd-4439-8b91-3b8795ac9456"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression(C=25, solver='liblinear')\n",
            "LogisticRegression-Accuracy: 0.6585365853658537\n",
            "RandomForestClassifier(n_estimators=25)\n",
            "RandomForest-Accuracy: 0.7073170731707317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Results"
      ],
      "metadata": {
        "id": "nYUeYfdzP3Yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Traditional Machine Learning Models\n",
        "\n",
        "**Best parameters for each model were:**\n",
        "\n",
        "```\n",
        "SVC:\n",
        "* best_params: {'C': 0.5, 'degree': 2, 'gamma': '1', 'kernel': 'poly'}\n",
        "\n",
        "LogisticRegression:\n",
        "* best_params: {'C': 25}\n",
        "\n",
        "RandomForests:\n",
        "* best_params: {'n_estimators': 25}\n",
        "```\n",
        "\n",
        "**Score for cross-validation were:**\n",
        "```\n",
        "SVC:\n",
        "* SVC-Mean: 0.7105555555555557\n",
        "* SVC-STD: 0.06445162795212402\n",
        "* SVC-Best: 0.8333333333333334\n",
        "\n",
        "Logistic Regression:\n",
        "* LogisticRegression-Mean: 0.7205555555555554\n",
        "* LogisticRegression-STD: 0.06836042823262703\n",
        "* LogisticRegression-Best: 0.8611111111111112\n",
        "\n",
        "Random Forest Classifier:\n",
        "* RandomForests-Mean: 0.7022222222222224\n",
        "* RandomForests-STD: 0.06359594676112972\n",
        "* RandomForests-Best: 0.8611111111111112\n",
        "\n",
        "* single: {'SVC': 43, 'LR': 43, 'RF': 32, 'TIE': 68}\n",
        "* mean: {'SVC': 4, 'LR': 4, 'RF': 2, 'TIE': 8}\n",
        "```\n",
        "\n",
        "**After testing the best estimators:**\n",
        "```\n",
        "Logistic Regression: ~0.683\n",
        "Random Forest Classifier: ~0.610\n",
        "```\n",
        "\n",
        "**After feature reduction, the cross validation results were:**\n",
        "```\n",
        "(None for SVC due to non-linear kernel mapping the features to higher dimension)\n",
        "\n",
        "LogisticRegression: (Nearly a 0.005 increase)\n",
        "* LogisticRegression-Mean: 0.7252777777777777\n",
        "* LogisticRegression-STD: 0.06222408231346678\n",
        "* LogisticRegression-Best: 0.8888888888888888\n",
        "* LogisticRegression-Threshold 1.5\n",
        "\n",
        "\n",
        "RandomForest (Nearly a 0.012 increase from before)\n",
        "* RandomForests-Mean: 0.7141666666666667\n",
        "* RandomForests-STD: 0.07610249342940639\n",
        "* RandomForests-Best: 0.8888888888888888\n",
        "* RandomForests-Threshold 0.009\n",
        "```\n",
        "\n",
        "**After testing with the best feature thresholds, testing results were:**\n",
        "```\n",
        "* LogisticRegression: ~0.659\n",
        "* RandomForest: ~0.707\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "PDPiab8NM-jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Learning Models\n",
        "\n",
        "**Best parameters for each model were:**\n",
        "\n",
        "```\n",
        "CNN:\n",
        "best_params: \n",
        "* {'learning_rate': ALWAYS 0.001\n",
        "  'convolutional_layers': ALWAYS 1,\n",
        "  'dropout': 0.05 (OR 1)\n",
        "  'kernels': 16 (OR 8)\n",
        "  'dense_layers': ALWAYS 1,\n",
        "  'dense_layer_kernels': ALWAYS 16} \n",
        "\n",
        "* 'epochs': ALWAYS 10\n",
        "* 'batch_size': 4 (OR 64)\n",
        "* 'early stop patience': 5 (for val_loss with val_split=0.1)\n",
        "\n",
        "LSTM:\n",
        "best_params: \n",
        "* {'learning_rate': ALWAYS 0.001\n",
        "  'lstm_layers': ALWAYS 1,\n",
        "  'units' : 32\n",
        "  'dropout': 0.05 (OR 0.1)\n",
        "  'dense_layers': ALWAYS 1,\n",
        "  'dense_layer_kernels': ALWAYS 16} \n",
        "\n",
        "* 'epochs': 20\n",
        "* 'batch_size': 4 (OR 64)\n",
        "* 'early stop patience': 5 (for val_loss with val_split=0.1)\n",
        "```\n",
        "\n",
        "**Score for cross-validation were:**\n",
        "```\n",
        "Accuracy of guessing just 0's 0.5722222222222222\n",
        "(given that the dataset was split with a stratified shuffle split)\n",
        "\n",
        "CNN:\n",
        "--TRAINING ACCURACY\n",
        "CNN-Mean: 0.5969444453716278\n",
        "CNN-STD: 0.06695481333821544\n",
        "CNN-Best: 0.7777777910232544\n",
        "--TRAINING LOSS\n",
        "CNN-Mean: 0.7707300865650177\n",
        "CNN-STD: 0.13538565023502167\n",
        "CNN-Best: 0.5461486577987671\n",
        "\n",
        "--TESTING ACCURACY\n",
        "CNN-Mean: 0.6878040680289268\n",
        "CNN-STD: 0.13067692963198604\n",
        "CNN-Best: 1.335808515548706\n",
        "-- TESTING LOSS\n",
        "CNN-Mean: 0.7556095570325851\n",
        "CNN-STD: 0.10480580317247869\n",
        "CNN-Best: 0.5419239401817322\n",
        "\n",
        "\n",
        "\n",
        "LSTM:\n",
        "-- TRAINING ACCURACY\n",
        "LSTM-Mean: 0.5713888713717461\n",
        "LSTM-STD: 0.04087299212067822\n",
        "LSTM-Best: 0.6111111044883728\n",
        "-- TRAINING LOSS\n",
        "LSTM-Mean: 0.6840356558561325\n",
        "LSTM-STD: 0.009066913945135709\n",
        "LSTM-Best: 0.6696506142616272\n",
        "\n",
        "-- TESTING ACCURACY\n",
        "LSTM-Mean: 0.5970178242027759\n",
        "LSTM-STD: 0.08834101820927136\n",
        "LSTM-Best: 0.7180540561676025\n",
        "-- TESTING LOSS\n",
        "LSTM-Mean: 0.7052804440259933\n",
        "LSTM-STD: 0.015844714917215407\n",
        "LSTM-Best: 0.6877207159996033\n",
        "```\n",
        "\n",
        "**After feature reduction, the best results were:**\n",
        "```\n",
        "There was no feature reduction\n",
        "```\n"
      ],
      "metadata": {
        "id": "qmFB8YBkP-WP"
      }
    }
  ]
}