{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oIAcB6Rkqjn3",
        "69zDFqfogbOd",
        "v5Z5wWUzqnzW",
        "N1KHbAcQhPbd"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS Prediction in VR\n",
        "\n",
        "## Author: \n",
        "###Tiago Manuel Severino Gon√ßalves. IST Lisboa, Portugal. Computer Engineering student number 89547"
      ],
      "metadata": {
        "id": "7J9J_84yDmJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library imports"
      ],
      "metadata": {
        "id": "w2ND5nXvqHjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all the necessary libraries\n",
        "\n",
        "* [numpy](https://numpy.org/): we will use it to store the data in array format for visualization\n",
        "* [pandas](https://pandas.pydata.org/): fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "built on top of the Python programming language. \n",
        "* [sklearn](https://scikit-learn.org/): provides a [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) implementation that we will use for visualization\n",
        "* [matplotlib](https://matplotlib.org/): plotting library for visualization\n",
        "* [tensorflow](https://www.tensorflow.org/): the neural network library"
      ],
      "metadata": {
        "id": "XMeKHpU4CwhM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4fX08RayCcVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae8c144-6a16-4211-f2de-c84ca68dd803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "tensorflow version 2.9.2\n",
            "sklearn version 1.0.2\n",
            "keras version 2.9.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, StratifiedShuffleSplit, RepeatedStratifiedKFold, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # Will later be deprecated and should be replaced with scikeras (https://github.com/adriangb/scikeras)\n",
        "\n",
        "import multiprocessing\n",
        "print(multiprocessing.cpu_count())\n",
        "\n",
        "rootFolder = \"/content/drive/MyDrive/Tese/\" #change this to better suit your needs\n",
        "\n",
        "print(\"tensorflow version\",tf.__version__)\n",
        "print(\"sklearn version\",sklearn.__version__)\n",
        "print(\"keras version\",tf.keras.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation\n",
        "\n",
        "#### In subsequent runs of the program, this step can be skipped during later testing, provided you have run it at least once as it saves both the labels and the features in a suitable format, such as csv or a readable numpy array"
      ],
      "metadata": {
        "id": "00PGM4trp-Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labels\n",
        "\n",
        "##### Now let's get all the valid experiment identifiers from FMS_tag.csv as well as the labels for each timestep of features"
      ],
      "metadata": {
        "id": "oIAcB6Rkqjn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(rootFolder+\"/CSV_Data/TAGS/FMS_Tag.csv\",\"r\") as fmsTags:\n",
        "  fmsTagsDict = {}\n",
        "\n",
        "  lines = fmsTags.readlines()\n",
        "\n",
        "  for line in lines:\n",
        "    labels = []\n",
        "    tokens = line.split(',')\n",
        "    for token in tokens[1:]:\n",
        "      labels.append(int(token))\n",
        "    fmsTagsDict[tokens[0]] = labels\n",
        "  \n",
        "keys = fmsTagsDict.keys()\n",
        "\n",
        "\"\"\" The following converts the dictionary into a digestible csv file, with every\n",
        "    empty entry being filled with an Nan\"\"\"\n",
        "fmsTagsDF = pd.DataFrame({key: pd.Series(value) for key, value in fmsTagsDict.items()})\n",
        "fmsTagsDF.to_csv(rootFolder+\"/CSV_Data/LABELS/labels.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "\"\"\" The following is how to get a list of the labels for a certain key (in this case, '0')\n",
        "    without all the NaN values. It also converts all the values to int which is what we want\"\"\"\n",
        "#temp = fmsTagsDF['0'].dropna().astype(int).tolist()\n",
        "\n",
        "\"\"\"\n",
        "# locate in the labelsDF a label for a given key (ex '0') and minute (ex 18)\n",
        "# print(int(labelsDF['0'].iloc[18]))\n",
        "\n",
        "# or use a dict from the labelsDF\n",
        "\n",
        "labelsDict = {}\n",
        "for columnName, columnData in labelsDF.iteritems():\n",
        "  labelsDict[columnName] = labelsDF[columnName].dropna().astype(int).tolist()\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gKBA00wNEveq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a68acd00-72fc-4372-85f4-351d930d9c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# locate in the labelsDF a label for a given key (ex '0') and minute (ex 18)\\n# print(int(labelsDF['0'].iloc[18]))\\n\\n# or use a dict from the labelsDF\\n\\nlabelsDict = {}\\nfor columnName, columnData in labelsDF.iteritems():\\n  labelsDict[columnName] = labelsDF[columnName].dropna().astype(int).tolist()\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video Processing\n",
        "\n",
        "##### Here we process the video to remove features for the different approaches of Machine Learning, tradional and deep learning."
      ],
      "metadata": {
        "id": "69zDFqfogbOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 #OpenCv package\n",
        "import os\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "from skimage.measure import shannon_entropy\n",
        "\n",
        "videosDirectory = rootFolder+\"/RECORDINGS/\"\n",
        "\n",
        "# Deep Learning features\n",
        "\n",
        "deepLearningList = []\n",
        "totalMinutes = 0\n",
        "\n",
        "for key in keys: # keys being the experiments that were considered valid\n",
        "    filename = key +'.mp4' \n",
        "    title = filename.split('.')[0]\n",
        "    filename = videosDirectory + '/' + filename\n",
        "    video = cv2.VideoCapture(filename)\n",
        "\n",
        "    frameCount = 0\n",
        "    frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "    duration = int(frames/fps)\n",
        "    timestep = 60\n",
        "    framesPerTimestep = fps * timestep\n",
        "        \n",
        "    print(filename,duration//60,frames)\n",
        "\n",
        "    width = int(video.get(3))\n",
        "    height = int(video.get(4))\n",
        "\n",
        "    # HSV initialization\n",
        "    hueList = []\n",
        "    satList = []\n",
        "    brightList = []\n",
        "\n",
        "    hue = {}\n",
        "    [hue.setdefault(i, 0) for i in range(32)] \n",
        "    sat = {}\n",
        "    [sat.setdefault(i, 0) for i in range(32)] \n",
        "    bright = {}\n",
        "    [bright.setdefault(i, 0) for i in range(32)] \n",
        "\n",
        "    # Motion Intensity Initialization\n",
        "\n",
        "    flowList = []\n",
        "    meanFlowList = []\n",
        "    stdDevFlowList = []\n",
        "    skewnessFlowList = []\n",
        "    kurtosisFlowList = []\n",
        "    maxFlowList = []\n",
        "    minFlowList = []\n",
        "    varianceFlowList = []\n",
        "\n",
        "    # Contrast Initialization\n",
        "\n",
        "    contrastList = []\n",
        "    meanContrastList = []\n",
        "    stdDevContrastList = []\n",
        "    skewnessContrastList = []\n",
        "    kurtosisContrastList = []\n",
        "    maxContrastList = []\n",
        "    minContrastList = []\n",
        "    varianceContrastList = []\n",
        "\n",
        "    # Entropy Initialization\n",
        "\n",
        "    entropyList = []\n",
        "    meanEntropyList = []\n",
        "    stdDevEntropyList = []\n",
        "    skewnessEntropyList = []\n",
        "    kurtosisEntropyList = []\n",
        "    maxEntropyList = []\n",
        "    minEntropyList = []\n",
        "    varianceEntropyList = []\n",
        "\n",
        "    while True:\n",
        "        elapsedSeconds = frameCount/fps\n",
        "        #print(\"frameCount:\",frameCount, \"  elapsedSeconds: \", elapsedSeconds)\n",
        "\n",
        "        ret, frameImg = video.read()\n",
        "        frameCount+=1\n",
        "\n",
        "        #if(elapsedSeconds > 120): break\n",
        "\n",
        "        if(elapsedSeconds%timestep == 0 and elapsedSeconds > 0):\n",
        "            print(\"     TIMESTEP: \", elapsedSeconds/timestep)\n",
        "\n",
        "            # HSV statistics\n",
        "            hueList.append(hue)\n",
        "            satList.append(sat)\n",
        "            brightList.append(bright)\n",
        "\n",
        "            # Motion Intensity statistics\n",
        "            meanFlowList.append(np.mean(flowList))\n",
        "            stdDevFlowList.append(np.std(flowList))\n",
        "            skewnessFlowList.append(skew(flowList))\n",
        "            kurtosisFlowList.append(kurtosis(flowList))\n",
        "            maxFlowList.append(max(flowList))\n",
        "            minFlowList.append(min(flowList))\n",
        "            varianceFlowList.append(np.var(flowList))\n",
        "\n",
        "            # Contrast statistics\n",
        "            meanContrastList.append(np.mean(contrastList))\n",
        "            stdDevContrastList.append(np.std(contrastList))\n",
        "            skewnessContrastList.append(skew(contrastList))\n",
        "            kurtosisContrastList.append(kurtosis(contrastList))\n",
        "            maxContrastList.append(max(contrastList))\n",
        "            minContrastList.append(min(contrastList))\n",
        "            varianceContrastList.append(np.var(contrastList))\n",
        "\n",
        "\n",
        "            # Entropy statistics\n",
        "            meanEntropyList.append(np.mean(entropyList))\n",
        "            stdDevEntropyList.append(np.std(entropyList))\n",
        "            skewnessEntropyList.append(skew(entropyList))\n",
        "            kurtosisEntropyList.append(kurtosis(entropyList))\n",
        "            maxEntropyList.append(max(entropyList))\n",
        "            minEntropyList.append(min(entropyList))\n",
        "            varianceEntropyList.append(np.var(entropyList))\n",
        "\n",
        "            # Deep Learning features\n",
        "\n",
        "            deepLearningDFTemp = pd.DataFrame([flowList,contrastList,entropyList])\n",
        "            deepLearningDFTemp = deepLearningDFTemp.transpose()\n",
        "            deepLearningDFTemp.columns=['Flow','Contrast','Entropy']\n",
        "\n",
        "            deepLearningList.append(deepLearningDFTemp.to_numpy())\n",
        "\n",
        "            deepLearningDFTemp = pd.DataFrame()\n",
        "\n",
        "            # Reset temporary variables\n",
        "\n",
        "            hue = {}\n",
        "            [hue.setdefault(i, 0) for i in range(32)] \n",
        "            sat = {}\n",
        "            [sat.setdefault(i, 0) for i in range(32)] \n",
        "            bright = {}\n",
        "            [bright.setdefault(i, 0) for i in range(32)]\n",
        "\n",
        "            flowList = []\n",
        "            contrastList = []\n",
        "            entropyList = []\n",
        "\n",
        "        if(elapsedSeconds >= duration): break\n",
        "\n",
        "        # Resize so that its less computationally intensive to process\n",
        "\n",
        "        frameImg = cv2.resize(frameImg, (128, 72))\n",
        "\n",
        "        # HSV Capture\n",
        "\n",
        "        hsv = cv2.cvtColor(frameImg, cv2.COLOR_BGR2HSV)\n",
        "        h,s,v = cv2.split(hsv)\n",
        "        for j in range(len(h)):\n",
        "            for k in range(len(h[j])):\n",
        "                hue[h[j][k]//8] += 1\n",
        "                sat[s[j][k]//8] += 1\n",
        "                bright[v[j][k]//8] += 1\n",
        "\n",
        "        # Motion Intensity Capture\n",
        "\n",
        "        if(elapsedSeconds > 0):\n",
        "            gray = cv2.cvtColor(frameImg, cv2.COLOR_BGR2GRAY)\n",
        "            flow = cv2.calcOpticalFlowFarneback(prevgray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            prevgray = gray\n",
        "\n",
        "            # We use the mean of the flow because why not and I don't think we should just use the entire flow for all timestep*fps frames\n",
        "            flowList.append(np.mean(flow))\n",
        "\n",
        "        else:\n",
        "            prevgray = cv2.cvtColor(frameImg, cv2.COLOR_BGR2GRAY)\n",
        "            flowList.append(0)\n",
        "\n",
        "        # Contrast & Entropy Capture\n",
        "        contrastList.append(prevgray.std()) #RMS contrast\n",
        "        entropyList.append(shannon_entropy(prevgray)) #Shannon Entropy\n",
        "\n",
        "        # Display\n",
        "        #cv2.imshow('frame', hsv)\n",
        "        #if(elapsedSeconds > 0):\n",
        "        #    cv2.imshow('flow', draw_flow(gray, flow))\n",
        "        #    cv2.imshow('flow HSV', draw_hsv(flow))\n",
        "\n",
        "        if cv2.waitKey(1) == ord(\"q\"):\n",
        "            break\n",
        "\n",
        "    # Create Data Frames\n",
        "\n",
        "    motionDataFrame = pd.DataFrame()\n",
        "    motionDataFrame['Mean'] = meanFlowList\n",
        "    motionDataFrame['Std'] = stdDevFlowList\n",
        "    motionDataFrame['Skweness'] = skewnessFlowList\n",
        "    motionDataFrame['Kurtosis'] = kurtosisFlowList\n",
        "    motionDataFrame['Max'] = maxFlowList\n",
        "    motionDataFrame['Min'] = minFlowList\n",
        "    motionDataFrame['Var'] = varianceFlowList\n",
        "\n",
        "    contrastDataFrame = pd.DataFrame()\n",
        "    contrastDataFrame['Mean'] = meanContrastList\n",
        "    contrastDataFrame['Std'] = stdDevContrastList\n",
        "    contrastDataFrame['Skweness'] = skewnessContrastList\n",
        "    contrastDataFrame['Kurtosis'] = kurtosisContrastList\n",
        "    contrastDataFrame['Max'] = maxContrastList\n",
        "    contrastDataFrame['Min'] = minContrastList\n",
        "    contrastDataFrame['Var'] = varianceContrastList\n",
        "\n",
        "    entropyDataFrame = pd.DataFrame()\n",
        "    entropyDataFrame['Mean'] = meanEntropyList\n",
        "    entropyDataFrame['Std'] = stdDevEntropyList\n",
        "    entropyDataFrame['Skweness'] = skewnessEntropyList\n",
        "    entropyDataFrame['Kurtosis'] = kurtosisEntropyList\n",
        "    entropyDataFrame['Max'] = maxEntropyList\n",
        "    entropyDataFrame['Min'] = minEntropyList\n",
        "    entropyDataFrame['Var'] = varianceEntropyList\n",
        "\n",
        "    hueDataFrame = pd.DataFrame.from_dict(hueList, orient='columns')\n",
        "\n",
        "    satDataFrame = pd.DataFrame.from_dict(satList, orient='columns')\n",
        "\n",
        "    brightDataFrame = pd.DataFrame.from_dict(brightList, orient='columns')\n",
        "\n",
        "\n",
        "    # Write to CSV\n",
        "    videoStatsDirectory = '/content/drive/MyDrive/Tese/CSV_Data/VIDEOS/'+title+'/'\n",
        "\n",
        "    motionDataFrame.to_csv(videoStatsDirectory+title+\"_motion.csv\")\n",
        "    contrastDataFrame.to_csv(videoStatsDirectory+title+\"_contrast.csv\")\n",
        "    entropyDataFrame.to_csv(videoStatsDirectory+title+\"_entropy.csv\")\n",
        "    hueDataFrame.to_csv(videoStatsDirectory+title+\"_hue.csv\")\n",
        "    satDataFrame.to_csv(videoStatsDirectory+title+\"_sat.csv\")\n",
        "    brightDataFrame.to_csv(videoStatsDirectory+title+\"_bright.csv\")\n",
        "\n",
        "    video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    print(\"ended at: \", elapsedSeconds,\"  Minutes: \", int(elapsedSeconds/60))\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "\n",
        "    totalMinutes += int(elapsedSeconds/60)\n",
        "\n",
        "deepLearningFeatures = np.array(deepLearningList)\n",
        "#print(deepLearningFeatures)\n",
        "print(deepLearningFeatures.shape)\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/Tese/CSV_Data/FEATURES/deepLearningFeatures\", deepLearningFeatures) #adds .npy file extension\n",
        "\n",
        "print(totalMinutes)\n",
        "\n",
        "#deepLearningFeatures = np.load(\"/content/drive/MyDrive/Tese/CSV_Data/FEATURES/deepLearningFeatures.npy\")"
      ],
      "metadata": {
        "id": "rxUpLJiVgzAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "449dcd76-4f57-4365-b68b-6bd895dd7f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Tese/RECORDINGS/0.mp4 29 3480.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "     TIMESTEP:  29.0\n",
            "ended at:  1740.0   Minutes:  29\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/1.mp4 20 2400.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "ended at:  1200.0   Minutes:  20\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/3.mp4 19 2280.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "ended at:  1140.0   Minutes:  19\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/7.mp4 18 2160.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "ended at:  1080.0   Minutes:  18\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/8.mp4 29 3480.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "     TIMESTEP:  29.0\n",
            "ended at:  1740.0   Minutes:  29\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/9.mp4 25 3000.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "ended at:  1500.0   Minutes:  25\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/10.mp4 28 3360.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "ended at:  1680.0   Minutes:  28\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/11.mp4 27 3240.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "ended at:  1620.0   Minutes:  27\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/12.mp4 26 3120.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "ended at:  1560.0   Minutes:  26\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/13.mp4 24 2880.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "ended at:  1440.0   Minutes:  24\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/14.mp4 26 3120.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "ended at:  1560.0   Minutes:  26\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/15.mp4 26 3120.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "ended at:  1560.0   Minutes:  26\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/19.mp4 34 4080.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "     TIMESTEP:  29.0\n",
            "     TIMESTEP:  30.0\n",
            "     TIMESTEP:  31.0\n",
            "     TIMESTEP:  32.0\n",
            "     TIMESTEP:  33.0\n",
            "     TIMESTEP:  34.0\n",
            "ended at:  2040.0   Minutes:  34\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/20.mp4 30 3600.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "     TIMESTEP:  29.0\n",
            "     TIMESTEP:  30.0\n",
            "ended at:  1800.0   Minutes:  30\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Tese/RECORDINGS/21.mp4 40 4800.0\n",
            "     TIMESTEP:  1.0\n",
            "     TIMESTEP:  2.0\n",
            "     TIMESTEP:  3.0\n",
            "     TIMESTEP:  4.0\n",
            "     TIMESTEP:  5.0\n",
            "     TIMESTEP:  6.0\n",
            "     TIMESTEP:  7.0\n",
            "     TIMESTEP:  8.0\n",
            "     TIMESTEP:  9.0\n",
            "     TIMESTEP:  10.0\n",
            "     TIMESTEP:  11.0\n",
            "     TIMESTEP:  12.0\n",
            "     TIMESTEP:  13.0\n",
            "     TIMESTEP:  14.0\n",
            "     TIMESTEP:  15.0\n",
            "     TIMESTEP:  16.0\n",
            "     TIMESTEP:  17.0\n",
            "     TIMESTEP:  18.0\n",
            "     TIMESTEP:  19.0\n",
            "     TIMESTEP:  20.0\n",
            "     TIMESTEP:  21.0\n",
            "     TIMESTEP:  22.0\n",
            "     TIMESTEP:  23.0\n",
            "     TIMESTEP:  24.0\n",
            "     TIMESTEP:  25.0\n",
            "     TIMESTEP:  26.0\n",
            "     TIMESTEP:  27.0\n",
            "     TIMESTEP:  28.0\n",
            "     TIMESTEP:  29.0\n",
            "     TIMESTEP:  30.0\n",
            "     TIMESTEP:  31.0\n",
            "     TIMESTEP:  32.0\n",
            "     TIMESTEP:  33.0\n",
            "     TIMESTEP:  34.0\n",
            "     TIMESTEP:  35.0\n",
            "     TIMESTEP:  36.0\n",
            "     TIMESTEP:  37.0\n",
            "     TIMESTEP:  38.0\n",
            "     TIMESTEP:  39.0\n",
            "     TIMESTEP:  40.0\n",
            "ended at:  2400.0   Minutes:  40\n",
            "----------------------------------------------------------------\n",
            "(401, 120, 3)\n",
            "401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features\n",
        "\n",
        "##### Now that every valid experiment and their labels are in an easily digestible format, we can start building the features. This will be done by merging every one of the pre-processed csv files contained in the CSV_Data folder into one single digestible format"
      ],
      "metadata": {
        "id": "v5Z5wWUzqnzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSSQ and SSQ stats\n",
        "statsMSSQ_DF = pd.read_csv(rootFolder+\"/CSV_Data/STATS/PROCESSED_MSSQ.csv\")\n",
        "#statsSSQ_DF = pd.read_csv(rootFolder+\"/CSV_Data/STATS/PROCESSED_SSQ.csv\")\n",
        "\n",
        "# Create a DataFrame to hold the features of all experiments \n",
        "allFeaturesDF = pd.DataFrame()\n",
        "\n",
        "labels = pd.read_csv(rootFolder+\"/CSV_Data/LABELS/labels.csv\")\n",
        "keys = labels.columns\n",
        "\n",
        "for key in keys:\n",
        "\n",
        "  # Create a DataFrame to hold the features of the experiment\n",
        "  featuresDF = pd.DataFrame()\n",
        "\n",
        "  # Adding Inputs features (first modifying the 'Minute' column as they start from 1)\n",
        "  inputsDF = pd.read_csv(rootFolder+\"/CSV_Data/INPUTS/INPUTS-TRAD/\"+key+\".csv\")\n",
        "\n",
        "  duration = inputsDF['Minute'].values[-1]\n",
        "  featuresDF['Minute'] = pd.Series(range(duration)) # Add Minute column, starting on 0\n",
        "  featuresDF.insert(0,'Label',fmsTagsDict[key])\n",
        "  featuresDF.insert(0,'Id',key) # Add Id column to the start\n",
        "\n",
        "  inputsDF = inputsDF.drop(columns=['Minute'])\n",
        "  featuresDF = pd.concat([featuresDF,inputsDF],axis=1)\n",
        "\n",
        "  # Adding Video features (first removing an unnecessary 'Unnamed 0:' column)\n",
        "  hueDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_hue.csv\").iloc[: , 1:]\n",
        "  satDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_sat.csv\").iloc[: , 1:]\n",
        "  brightDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_bright.csv\").iloc[: , 1:]\n",
        "\n",
        "  \"\"\" The less error prone way to do this would be to do 3 seperate loops for each\n",
        "      color dataframe's columns since they could be different. But they aren't. \n",
        "      So I just use the same columns of hueDF for all 3 of them\"\"\"\n",
        "  for columnName, columnData in hueDF.iteritems():\n",
        "    hueDF.rename(columns = {columnName:\"hue_\"+columnName}, inplace = True)\n",
        "    satDF.rename(columns = {columnName:\"sat_\"+columnName}, inplace = True)\n",
        "    brightDF.rename(columns = {columnName:\"bright_\"+columnName}, inplace = True)\n",
        "\n",
        "  featuresDF = pd.concat([featuresDF,hueDF],axis=1)\n",
        "  featuresDF = pd.concat([featuresDF,satDF],axis=1)\n",
        "  featuresDF = pd.concat([featuresDF,brightDF],axis=1)\n",
        "\n",
        "  motionDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_motion.csv\").iloc[: , 1:]\n",
        "  contrastDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_contrast.csv\").iloc[: , 1:]\n",
        "  entropyDF = pd.read_csv(rootFolder+\"/CSV_Data/VIDEOS/\"+key+\"/\"+key+\"_entropy.csv\").iloc[: , 1:]\n",
        "\n",
        "  \"\"\" Same as before, I know they have the same columns, so I can just do this\"\"\"\n",
        "  for columnName, columnData in motionDF.iteritems():\n",
        "    motionDF.rename(columns = {columnName:\"motion_\"+columnName}, inplace = True)\n",
        "    contrastDF.rename(columns = {columnName:\"contrast_\"+columnName}, inplace = True)\n",
        "    entropyDF.rename(columns = {columnName:\"entropy_\"+columnName}, inplace = True)\n",
        "\n",
        "  featuresDF = pd.concat([featuresDF,motionDF],axis=1)\n",
        "  featuresDF = pd.concat([featuresDF,contrastDF],axis=1)\n",
        "  featuresDF = pd.concat([featuresDF,entropyDF],axis=1)\n",
        "\n",
        "  # Adding Stats features\n",
        "  mssqRawScore = statsMSSQ_DF[statsMSSQ_DF[\"Id\"] == int(key)][\"MSSQ Raw score\"].iloc[0]\n",
        "  #ssqTotalScore = statsSSQ_DF[statsSSQ_DF[\"Id\"] == int(key)][\"TotalScore\"].iloc[0]\n",
        "\n",
        "  featuresDF[\"MSSQ\"] = mssqRawScore\n",
        "  #featuresDF[\"SSQ\"] = ssqTotalScore\n",
        "\n",
        "  filename = rootFolder+\"/CSV_Data/FEATURES/\" + key + \".csv\"\n",
        "  print(filename)\n",
        "  print(featuresDF.shape)\n",
        "  featuresDF.to_csv(filename, encoding='utf-8', index=False)\n",
        "\n",
        "  allFeaturesDF = pd.concat([allFeaturesDF, featuresDF])\n",
        "\n",
        "datasetDF = allFeaturesDF.drop(columns=['Minute','Id'])\n",
        "allFeaturesDF = allFeaturesDF.drop(columns=['Label'])\n",
        "\n",
        "filename = rootFolder+\"/CSV_Data/FEATURES/allFeatures.csv\"\n",
        "print(filename)\n",
        "print(allFeaturesDF.shape)\n",
        "allFeaturesDF.to_csv(filename, encoding='utf-8', index=False)\n",
        "\n",
        "filename = rootFolder+\"/CSV_Data/FEATURES/dataset.csv\"\n",
        "print(filename)\n",
        "print(datasetDF.shape)\n",
        "datasetDF.to_csv(filename, encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "K2sqqoEjo2Mk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461b2690-7dfd-4bcf-c0a2-25aa15c236c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/0.csv\n",
            "(29, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/1.csv\n",
            "(20, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/3.csv\n",
            "(19, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/7.csv\n",
            "(18, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/8.csv\n",
            "(29, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/9.csv\n",
            "(25, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/10.csv\n",
            "(28, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/11.csv\n",
            "(27, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/12.csv\n",
            "(26, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/13.csv\n",
            "(24, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/14.csv\n",
            "(26, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/15.csv\n",
            "(26, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/19.csv\n",
            "(34, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/20.csv\n",
            "(30, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/21.csv\n",
            "(40, 127)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/allFeatures.csv\n",
            "(401, 126)\n",
            "/content/drive/MyDrive/Tese/CSV_Data/FEATURES/dataset.csv\n",
            "(401, 125)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the video features we generated before\n",
        "videoDeepLearningFeatures = np.load(\"/content/drive/MyDrive/Tese/CSV_Data/FEATURES/videoDeepLearningFeatures.npy\")\n",
        "\n",
        "# Create an array to hold the input features of all experiments \n",
        "allInputs = []\n",
        "\n",
        "labels = pd.read_csv(\"/content/drive/MyDrive/Tese/CSV_Data/LABELS/labels.csv\")\n",
        "keys = labels.columns\n",
        "\n",
        "for key in keys:\n",
        "  # Getting input features dataframe for each valid experiment\n",
        "  inputsDF = pd.read_csv(\"/content/drive/MyDrive/Tese/CSV_Data/INPUTS/INPUTS-DEEP/\"+key+\".csv\")\n",
        "\n",
        "  inputsDF = inputsDF.drop(columns=['ElapsedSeconds'])\n",
        "\n",
        "  # iterate over each 120 input instances for this input data\n",
        "  inputs = []\n",
        "  for i in range(0,len(inputsDF)-1,120):\n",
        "    input = inputsDF.iloc[i:i+120].to_numpy()\n",
        "    inputs.append(input)\n",
        "\n",
        "  # Stack them all so we have a (<number_of_minutes>,120,7)\n",
        "  inputs = np.stack(inputs,axis=0)\n",
        "\n",
        "  allInputs.append(inputs)\n",
        "\n",
        "# Join all the input features\n",
        "allInputs = np.concatenate(allInputs,axis=0)\n",
        "\n",
        "# Join all the deep learning features\n",
        "deepFeatures = np.concatenate((allInputs,videoDeepLearningFeatures),axis=2)\n",
        "\n",
        "print(deepFeatures.shape)\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/Tese/CSV_Data/FEATURES/deepLearningFeatures\", deepFeatures) #adds .npy file extension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KF-cz8erhsh",
        "outputId": "027b65aa-09d4-4c48-967c-c34f26aa8ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(401, 120, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning\n",
        "\n",
        "#### Actually using the data we obtained"
      ],
      "metadata": {
        "id": "y7jUKJYCHfRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features and Labels Preperation\n",
        "\n"
      ],
      "metadata": {
        "id": "N1KHbAcQhPbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Learning Features and Labels"
      ],
      "metadata": {
        "id": "lMrxyxgufpbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Features and the Labels\n",
        "\n",
        "datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/TradFeaturesAndLabels.csv\")\n",
        "\n",
        "deep_y = datasetDF['Label'].to_numpy()\n",
        "deep_X = np.load(rootFolder+\"/CSV_Data/FEATURES/deepLearningFeatures.npy\")\n",
        "\n",
        "deep_X = tf.keras.utils.normalize(deep_X, axis=1)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Preparing the datasets:\n",
        "# 90% for training and 10% for testing\n",
        "deep_X_train, deep_X_test, deep_y_train, deep_y_test = train_test_split(deep_X, deep_y, test_size=0.10, shuffle=True, random_state=39)\n",
        "\n",
        "#sizes = datasetDF['Label'].value_counts(sort=1)\n",
        "#plt.pie(sizes,autopct='%1.1f%%') #60% vs 40%\n",
        "\n",
        "print(\"Classes\")\n",
        "print(datasetDF.iloc[:,0].value_counts()) # classes\n",
        "\n",
        "print(\"Features\")\n",
        "print(deep_y.shape) # 433 labels\n",
        "print(deep_X.shape) # (433,120,3) features\n",
        "\n",
        "# Show how many 0's and 1's are in the training and testing data\n",
        "print(\"0:\", np.count_nonzero(deep_y_train == 0), \"1:\", np.count_nonzero(deep_y_train == 1))\n",
        "print(\"0:\", np.count_nonzero(deep_y_test == 0), \"1:\", np.count_nonzero(deep_y_test == 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBkkDxmCfubY",
        "outputId": "6caed2fa-a369-4f62-b64f-04c2fbdfc63f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes\n",
            "0    229\n",
            "1    172\n",
            "Name: Label, dtype: int64\n",
            "Features\n",
            "(401,)\n",
            "(401, 120, 10)\n",
            "0: 208 1: 152\n",
            "0: 21 1: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Traditional Learning Features and Labels"
      ],
      "metadata": {
        "id": "rDnWW7mBfu-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Features and the Labels\n",
        "\n",
        "datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/TradFeaturesAndLabels.csv\")\n",
        "\n",
        "trad_y = datasetDF['Label'].to_numpy()\n",
        "trad_X = datasetDF.drop(columns=['Label']).to_numpy()\n",
        "\n",
        "trad_X = tf.keras.utils.normalize(trad_X, axis=1)\n",
        "\n",
        "print(trad_X.shape)\n",
        "\n",
        "\n",
        "# Preparing the datasets:\n",
        "# 90% for training and 10% for testing\n",
        "trad_X_train, trad_X_test, trad_y_train, trad_y_test = train_test_split(trad_X, trad_y, test_size=0.10, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Classes\")\n",
        "print(datasetDF.iloc[:,0].value_counts()) # classes\n",
        "\n",
        "print(\"Features\")\n",
        "print(trad_y.shape) # 433 labels\n",
        "print(trad_X.shape) # 433*118 features"
      ],
      "metadata": {
        "id": "Q-Rx1RPmHmvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972857aa-c892-4829-9363-04f7207587e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(401, 124)\n",
            "Classes\n",
            "0    229\n",
            "1    172\n",
            "Name: Label, dtype: int64\n",
            "Features\n",
            "(401,)\n",
            "(401, 124)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Learning Models\n",
        "\n",
        "##### Here we will use a variety of machine learning models and compare results"
      ],
      "metadata": {
        "id": "lvL2k9T8F2yR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Learning Models\n",
        "\n",
        "###### Here we will use a variety of machine learning models and compare results"
      ],
      "metadata": {
        "id": "35HuWMWtex_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for CNN models\n",
        "\n",
        "def define_CNN( learning_rate=0.001, \n",
        "                convolutional_layers=1, \n",
        "                dropout=0.05, \n",
        "                kernels=16, \n",
        "                dense_layers=1, \n",
        "                dense_layer_kernels=16,\n",
        "                kernel_size=(3,3), \n",
        "                input_shape=(120,10,1), \n",
        "                pool_size=(2,2), \n",
        "                batch_normalization=False):\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  for i in range(convolutional_layers):\n",
        "    if(i == 0):\n",
        "      model.add(tf.keras.layers.Conv2D(kernels, kernel_size, activation='relu', strides=(1, 1), padding='same', input_shape=input_shape))\n",
        "    else:\n",
        "      model.add(tf.keras.layers.Conv2D(kernels, kernel_size, activation='relu', strides=(1, 1), padding='same'))\n",
        "    \n",
        "    if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=pool_size, padding='same'))\n",
        "\n",
        "    if(dropout > 0):\n",
        "      model.add(tf.keras.layers.Dropout(rate=dropout))\n",
        "  \n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  for i in range(dense_layers):\n",
        "    model.add(tf.keras.layers.Dense(dense_layer_kernels, activation='relu'))\n",
        "\n",
        "    if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[\"accuracy\"])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "euu6MtX3Sb5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for LSTM-RNN models \n",
        "\n",
        "def define_LSTM( learning_rate=0.001, \n",
        "                 lstm_layers=1,\n",
        "                 units=10,\n",
        "                 dropout=0.05, \n",
        "                 dense_layers=1, \n",
        "                 dense_layer_kernels=16,\n",
        "                 input_shape=(120,10), \n",
        "                 batch_normalization=False):\n",
        "  \n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  for i in range(lstm_layers):\n",
        "    if(i == 0):\n",
        "      if(i == lstm_layers-1):\n",
        "        model.add(tf.keras.layers.LSTM(units, activation='relu', input_shape=input_shape))\n",
        "      else:\n",
        "        model.add(tf.keras.layers.LSTM(units, activation='relu', return_sequences=True, input_shape=input_shape))\n",
        "\n",
        "    elif(i == lstm_layers-1):\n",
        "      model.add(tf.keras.layers.LSTM(units, activation='relu'))\n",
        "    else:\n",
        "      model.add(tf.keras.layers.LSTM(units, activation='relu', return_sequences=True))\n",
        "    \n",
        "    if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if(dropout > 0):\n",
        "      model.add(tf.keras.layers.Dropout(rate=dropout))\n",
        "  \n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  for i in range(dense_layers):\n",
        "    model.add(tf.keras.layers.Dense(dense_layer_kernels, activation='relu'))\n",
        "\n",
        "    if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if(batch_normalization):\n",
        "      model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[\"accuracy\"])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "GKjSZxZPZBTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### GridSearch the best parameters for CNN and LSTM"
      ],
      "metadata": {
        "id": "ZN3qlAcfSar9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### CNN "
      ],
      "metadata": {
        "id": "a9TIuU62Re1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search CNN\n",
        "\n",
        "scores = []\n",
        "totalScores = []\n",
        "\n",
        "sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10,random_state=42)\n",
        "batch_size = [None,4,8,16,32,64]\n",
        "epochs =  [7,9,10]\n",
        "\n",
        "# We'll see that if something performs better with the last entry of a given list,\n",
        "# we'll check if it perfoms better with the one after\n",
        "\n",
        "learning_rate = [0.001,0.0001]\n",
        "convolutional_layers=[1,2] # so for this one, if it perfoms better with 2, we'll try with 3 and so on\n",
        "dropout=[0.0,0.05,0.10]\n",
        "kernels=[8,16,32] # same for the other direction. if it performs better with 8, we'll try 4\n",
        "dense_layers=[1,2]\n",
        "dense_layer_kernels=[8,16,32]\n",
        "\n",
        "param_grid = dict(learning_rate=learning_rate,\n",
        "                  convolutional_layers=convolutional_layers,\n",
        "                  dropout=dropout,\n",
        "                  kernels=kernels,\n",
        "                  dense_layers=dense_layers,\n",
        "                  dense_layer_kernels=dense_layer_kernels)\n",
        "\n",
        "\n",
        "cnn_X = deep_X.reshape((deep_X.shape[0],deep_X.shape[1],deep_X.shape[2],1))\n",
        "\n",
        "for epoch in epochs:\n",
        "  for batch in batch_size:\n",
        "    model = KerasClassifier(build_fn = define_CNN,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch,\n",
        "                                verbose=1)\n",
        "    clf = GridSearchCV(estimator=model,param_grid=param_grid,cv=sssplit, return_train_score=False)\n",
        "    clf.fit(cnn_X,deep_y)\n",
        "    scores.append({\n",
        "        'model' : 'CNN',\n",
        "        'best_score' : clf.best_score_,\n",
        "        'best_params' : clf.best_params_,\n",
        "        'epochs' : epoch,\n",
        "        'batch_size' : batch\n",
        "    })\n",
        "\n",
        "    print(scores)\n",
        "    totalScores.append(scores)\n",
        "\n",
        "best_score = None\n",
        "maxScore = 0\n",
        "\n",
        "for Scores in totalScores:\n",
        "  for score in Scores:\n",
        "    if score['best_score'] >= maxScore:\n",
        "      best_score = score\n",
        "\n",
        "best_score"
      ],
      "metadata": {
        "id": "mGg8jhya_Bgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### LSTM"
      ],
      "metadata": {
        "id": "tQCLZk89Rg93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search LSTM\n",
        "\n",
        "scores = []\n",
        "totalScores = []\n",
        "\n",
        "sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10,random_state=42)\n",
        "batch_size = [None,4,8,16,32,64]\n",
        "epochs =  [5,10,20]\n",
        "\n",
        "# We'll see that if something performs better with the last entry of a given list,\n",
        "# we'll check if it perfoms better with the one after\n",
        "\n",
        "learning_rate = [0.001,0.0001]\n",
        "units = [10,25,50,75,100]\n",
        "lstm_layers=[1,2] # so for this one, if it perfoms better with 2, we'll try with 3 and so on\n",
        "dropout=[0.0,0.05,0.10]\n",
        "dense_layers=[1,2]\n",
        "dense_layer_kernels=[8,16,32]\n",
        "\n",
        "param_grid = dict(learning_rate=learning_rate,\n",
        "                  units=units,\n",
        "                  lstm_layers=lstm_layers,\n",
        "                  dropout=dropout,\n",
        "                  dense_layers=dense_layers,\n",
        "                  dense_layer_kernels=dense_layer_kernels)\n",
        "\n",
        "\n",
        "for epoch in epochs:\n",
        "  for batch in batch_size:\n",
        "    model = KerasClassifier(build_fn = define_LSTM,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch,\n",
        "                                verbose=1)\n",
        "    clf = GridSearchCV(estimator=model,param_grid=param_grid,cv=sssplit,return_train_score=False,verbose=1)\n",
        "    clf.fit(deep_X,deep_y)\n",
        "    scores.append({\n",
        "        'model' : 'CNN',\n",
        "        'best_score' : clf.best_score_,\n",
        "        'best_params' : clf.best_params_,\n",
        "        'epochs' : epoch,\n",
        "        'batch_size' : batch\n",
        "    })\n",
        "\n",
        "    print(scores)\n",
        "    totalScores.append(scores)\n",
        "\n",
        "best_score = None\n",
        "maxScore = 0\n",
        "\n",
        "for Scores in totalScores:\n",
        "  for score in Scores:\n",
        "    if score['best_score'] >= maxScore:\n",
        "      best_score = score\n",
        "\n",
        "best_score"
      ],
      "metadata": {
        "id": "StlWV2YgKu0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cross Validation "
      ],
      "metadata": {
        "id": "k_qz67QvVkNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### CNN "
      ],
      "metadata": {
        "id": "xiU3ZB01F-30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "From the GridSearch, we learned:\n",
        "\n",
        "36 models achieved a 'best_score': 0.6585365891456604\n",
        "\n",
        "'best_params': \n",
        "\n",
        "{\n",
        "  'learning_rate': ALWAYS 0.001\n",
        "  'convolutional_layers': ALWAYS 1,\n",
        "  'dropout': 0.05 OR 0.1\n",
        "  'kernels': 8 OR 16\n",
        "  'dense_layers': ALWAYS 1,\n",
        "  'dense_layer_kernels': ALWAYS 16,\n",
        "}, \n",
        "\n",
        "'epochs': ALWAYS 10\n",
        "'batch_size': 4 OR 64\n",
        "\"\"\"\n",
        "\n",
        "# set early stopping criteria\n",
        "pat = 5 #this is the number of epochs with no improvment after which the training will stop\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
        "\n",
        "# define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
        "#model_checkpoint = tf.keras.callbacks.ModelCheckpoint('fas_mnist_1.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "# reshape the feature dataset so it satisfies CNN input\n",
        "cnn_X = deep_X.reshape((deep_X.shape[0],deep_X.shape[1],deep_X.shape[2],1))\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 4\n",
        "numLoops = 10\n",
        "verbose = 0\n",
        "\n",
        "total_results = []\n",
        "total_losses = []\n",
        "\n",
        "# Run a StratifiedShuffleSplit 10 times on the dataset to approximate randomness\n",
        "\n",
        "for i in range(numLoops):\n",
        "  print(i)\n",
        "\n",
        "  sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10)\n",
        "\n",
        "  results = []\n",
        "  losses = []\n",
        "  for train, test in sssplit.split(cnn_X, deep_y):\n",
        "    # Create the model with the best parameters\n",
        "    model = define_CNN(learning_rate=0.001, \n",
        "                     convolutional_layers=1, \n",
        "                     dropout=0.1, \n",
        "                     kernels=16, \n",
        "                     dense_layers=1, \n",
        "                     dense_layer_kernels=16,\n",
        "                     kernel_size=(3,3), \n",
        "                     input_shape=(120,10,1), \n",
        "                     pool_size=(2,2), \n",
        "                     batch_normalization=False)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(cnn_X[train], deep_y[train], epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stopping],  verbose=verbose)\n",
        "\n",
        "    # evaluate the model\n",
        "    scores = model.evaluate(cnn_X[test], deep_y[test], verbose=verbose)\n",
        "    #print(model.metrics_names[0], scores[0], model.metrics_names[1], scores[1])\n",
        "\n",
        "    results.append(scores[1])\n",
        "    losses.append(scores[0])\n",
        "\n",
        "  #print(\"CNN-Mean - \", \"loss:\", np.mean(losses), \"accuracy:\", np.mean(results))\n",
        "  #print(\"CNN-STD - \", \"loss:\", np.std(losses), \"accuracy:\", np.std(results))\n",
        "  #print(\"CNN-Best - \", \"min-loss:\", np.amin(losses), \"max-accuracy:\", np.amax(results))\n",
        "\n",
        "  total_results.append(results)\n",
        "  total_losses.append(losses)\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in total_results:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"CNN-ACCURACY\")\n",
        "print(\"CNN-Mean:\",np.mean(totals))\n",
        "print(\"CNN-STD:\",np.std(totals))\n",
        "print(\"CNN-Best:\",np.amax(totals))\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in total_losses:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"CNN-LOSS\")\n",
        "print(\"CNN-Mean:\",np.mean(totals))\n",
        "print(\"CNN-STD:\",np.std(totals))\n",
        "print(\"CNN-Best:\",np.amin(totals))"
      ],
      "metadata": {
        "id": "2dJ0gSyxdRmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### LSTM"
      ],
      "metadata": {
        "id": "C-77k5qLVWPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set early stopping criteria\n",
        "pat = 5 #this is the number of epochs with no improvment after which the training will stop\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
        "\n",
        "# define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
        "#model_checkpoint = tf.keras.callbacks.ModelCheckpoint('fas_mnist_1.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "numLoops = 10\n",
        "verbose = 0\n",
        "\n",
        "total_results = []\n",
        "total_losses = []\n",
        "\n",
        "# Run a StratifiedShuffleSplit 10 times on the dataset to approximate randomness\n",
        "\n",
        "for i in range(numLoops):\n",
        "  print(i)\n",
        "\n",
        "  sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10)\n",
        "\n",
        "  results = []\n",
        "  losses = []\n",
        "  for train, test in sssplit.split(deep_X, deep_y):\n",
        "    # Create the model with the best parameters\n",
        "    model = define_LSTM(  learning_rate=0.001, \n",
        "                          lstm_layers=1,\n",
        "                          units=32,\n",
        "                          dropout=0.05, \n",
        "                          dense_layers=1, \n",
        "                          dense_layer_kernels=16,\n",
        "                          input_shape=(120,10), \n",
        "                          batch_normalization=True)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(deep_X[train], deep_y[train], epochs=epochs, batch_size=batch_size, validation_split=0.1,  verbose=verbose)\n",
        "\n",
        "    # evaluate the model\n",
        "    scores = model.evaluate(deep_X[test], deep_y[test], verbose=verbose)\n",
        "    print(model.metrics_names[0], scores[0], model.metrics_names[1], scores[1])\n",
        "\n",
        "    results.append(scores[1])\n",
        "    losses.append(scores[0])\n",
        "\n",
        "  #print(\"LSTM-Mean - \", \"loss:\", np.mean(losses), \"accuracy:\", np.mean(results))\n",
        "  #print(\"LSTM-STD - \", \"loss:\", np.std(losses), \"accuracy:\", np.std(results))\n",
        "  #print(\"LSTM-Best - \", \"min-loss:\", np.amin(losses), \"max-accuracy:\", np.amax(results))\n",
        "\n",
        "  total_results.append(results)\n",
        "  total_losses.append(losses)\n",
        "\n",
        "  # Show how many 0's and 1's are in the training and testing data\n",
        "  print(\"Accuracy of guessing just 0's\",np.count_nonzero(deep_y[train] == 0)/(np.count_nonzero(deep_y[train] == 0)+np.count_nonzero(deep_y[train] == 1)))\n",
        "  print(\"Accuracy of guessing just 0's\",np.count_nonzero(deep_y[train] == 0)/(np.count_nonzero(deep_y[train] == 0)+np.count_nonzero(deep_y[train] == 1)))\n",
        "\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in total_results:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"LSTM-ACCURACY\")\n",
        "print(\"LSTM-Mean:\",np.mean(totals))\n",
        "print(\"LSTM-STD:\",np.std(totals))\n",
        "print(\"LSTM-Best:\",np.amax(totals))\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in total_losses:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"LSTM-LOSS\")\n",
        "print(\"LSTM-Mean:\",np.mean(totals))\n",
        "print(\"LSTM-STD:\",np.std(totals))\n",
        "print(\"LSTM-Best:\",np.amin(totals))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNxf7h4qQwBX",
        "outputId": "8707517c-6191-4513-efe2-735f0f483fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "loss 0.6880882978439331 accuracy 0.5609756112098694\n",
            "loss 0.6878853440284729 accuracy 0.5609756112098694\n",
            "loss 0.686397910118103 accuracy 0.5609756112098694\n",
            "loss 0.6891376972198486 accuracy 0.5609756112098694\n",
            "loss 0.684434711933136 accuracy 0.5609756112098694\n",
            "loss 0.684076189994812 accuracy 0.5609756112098694\n",
            "loss 0.6864786148071289 accuracy 0.5609756112098694\n",
            "loss 0.6817284822463989 accuracy 0.5609756112098694\n",
            "loss 0.6818827390670776 accuracy 0.5609756112098694\n",
            "loss 0.6931195855140686 accuracy 0.5609756112098694\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "1\n",
            "loss 0.6881238222122192 accuracy 0.5609756112098694\n",
            "loss 0.6864941120147705 accuracy 0.5609756112098694\n",
            "loss 0.6970040202140808 accuracy 0.5609756112098694\n",
            "loss 0.6827999353408813 accuracy 0.5609756112098694\n",
            "loss 0.6838153004646301 accuracy 0.5853658318519592\n",
            "loss 0.6832473278045654 accuracy 0.5609756112098694\n",
            "loss 0.6969470381736755 accuracy 0.5121951103210449\n",
            "loss 0.681324303150177 accuracy 0.5853658318519592\n",
            "loss 0.6809573173522949 accuracy 0.5609756112098694\n",
            "loss 0.685937225818634 accuracy 0.5609756112098694\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "2\n",
            "loss 0.6848099231719971 accuracy 0.5609756112098694\n",
            "loss 0.6848461031913757 accuracy 0.5609756112098694\n",
            "loss 0.6808102130889893 accuracy 0.5609756112098694\n",
            "loss 0.6956741809844971 accuracy 0.5609756112098694\n",
            "loss 0.6844570636749268 accuracy 0.5609756112098694\n",
            "loss 0.7015033960342407 accuracy 0.5609756112098694\n",
            "loss 0.6892709732055664 accuracy 0.5609756112098694\n",
            "loss 0.6858449578285217 accuracy 0.5609756112098694\n",
            "loss 0.6981596350669861 accuracy 0.5609756112098694\n",
            "loss 0.6874988675117493 accuracy 0.5609756112098694\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "3\n",
            "loss 0.6925652027130127 accuracy 0.5609756112098694\n",
            "loss 0.685583770275116 accuracy 0.5609756112098694\n",
            "loss 0.6860660314559937 accuracy 0.5609756112098694\n",
            "loss 0.701735258102417 accuracy 0.4390243887901306\n",
            "loss 0.6796178221702576 accuracy 0.5609756112098694\n",
            "loss 0.6874920129776001 accuracy 0.5609756112098694\n",
            "loss 0.7104248404502869 accuracy 0.5609756112098694\n",
            "loss 0.6821776032447815 accuracy 0.5853658318519592\n",
            "loss 0.6886950135231018 accuracy 0.5609756112098694\n",
            "loss 0.6812822222709656 accuracy 0.5609756112098694\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "4\n",
            "loss 0.6855101585388184 accuracy 0.5609756112098694\n",
            "loss 0.6843249797821045 accuracy 0.5609756112098694\n",
            "loss 0.6927222013473511 accuracy 0.5609756112098694\n",
            "loss 0.7012786865234375 accuracy 0.5609756112098694\n",
            "loss 0.68548583984375 accuracy 0.5609756112098694\n",
            "loss 0.6871446371078491 accuracy 0.5609756112098694\n",
            "loss 0.7030729055404663 accuracy 0.5609756112098694\n",
            "loss 0.6931952238082886 accuracy 0.5609756112098694\n",
            "loss 0.7056013941764832 accuracy 0.3658536672592163\n",
            "loss 0.6812622547149658 accuracy 0.5609756112098694\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "5\n",
            "loss 0.6912349462509155 accuracy 0.5609756112098694\n",
            "loss 0.7262169718742371 accuracy 0.5609756112098694\n",
            "loss 0.7060108780860901 accuracy 0.5609756112098694\n",
            "loss 0.6796004772186279 accuracy 0.5609756112098694\n",
            "loss 0.6957546472549438 accuracy 0.5365853905677795\n",
            "loss 0.6885205507278442 accuracy 0.5609756112098694\n",
            "loss 0.6875391006469727 accuracy 0.5609756112098694\n",
            "loss 0.683384120464325 accuracy 0.5609756112098694\n",
            "loss 0.6867311000823975 accuracy 0.5609756112098694\n",
            "loss 0.7229423522949219 accuracy 0.5609756112098694\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "6\n",
            "loss 0.6900171041488647 accuracy 0.5609756112098694\n",
            "loss 0.6896876692771912 accuracy 0.6097561120986938\n",
            "loss 0.6853345632553101 accuracy 0.5609756112098694\n",
            "loss 0.6974175572395325 accuracy 0.5609756112098694\n",
            "loss 0.6947814226150513 accuracy 0.5609756112098694\n",
            "loss 0.6787143349647522 accuracy 0.5609756112098694\n",
            "loss 0.6836305260658264 accuracy 0.5609756112098694\n",
            "loss 0.6966096758842468 accuracy 0.5609756112098694\n",
            "loss 0.6793781518936157 accuracy 0.5609756112098694\n",
            "loss 0.6894776821136475 accuracy 0.5609756112098694\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "7\n",
            "loss 0.6924037337303162 accuracy 0.4878048896789551\n",
            "loss 0.6940551400184631 accuracy 0.5609756112098694\n",
            "loss 0.6804497838020325 accuracy 0.5609756112098694\n",
            "loss 0.6826168298721313 accuracy 0.5365853905677795\n",
            "loss 0.6904603838920593 accuracy 0.5609756112098694\n",
            "loss 0.6878809332847595 accuracy 0.5609756112098694\n",
            "loss 0.6906562447547913 accuracy 0.5609756112098694\n",
            "loss 0.6764761209487915 accuracy 0.5609756112098694\n",
            "loss 0.6961104869842529 accuracy 0.5609756112098694\n",
            "loss 0.6778241991996765 accuracy 0.5609756112098694\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "8\n",
            "loss 0.746684730052948 accuracy 0.5609756112098694\n",
            "loss 0.6807412505149841 accuracy 0.5853658318519592\n",
            "loss 0.7026028037071228 accuracy 0.5609756112098694\n",
            "loss 0.6935686469078064 accuracy 0.5609756112098694\n",
            "loss 0.6958835124969482 accuracy 0.5609756112098694\n",
            "loss 0.6916422247886658 accuracy 0.5609756112098694\n",
            "loss 0.6963292360305786 accuracy 0.4146341383457184\n",
            "loss 0.6865667700767517 accuracy 0.5609756112098694\n",
            "loss 0.7175655364990234 accuracy 0.5609756112098694\n",
            "loss 0.6930021643638611 accuracy 0.6097561120986938\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "9\n",
            "loss 0.6894591450691223 accuracy 0.6341463327407837\n",
            "loss 0.6906441450119019 accuracy 0.5609756112098694\n",
            "loss 0.7009743452072144 accuracy 0.5609756112098694\n",
            "loss 0.6839789152145386 accuracy 0.5609756112098694\n",
            "loss 0.6867706775665283 accuracy 0.5609756112098694\n",
            "loss 0.6761513352394104 accuracy 0.5853658318519592\n",
            "loss 0.6907303929328918 accuracy 0.5609756112098694\n",
            "loss 0.7062124609947205 accuracy 0.5609756112098694\n",
            "loss 0.7055589556694031 accuracy 0.5609756112098694\n",
            "loss 0.6832032799720764 accuracy 0.5609756112098694\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "Accuracy of guessing just 0's 0.5722222222222222\n",
            "LSTM-ACCURACY\n",
            "LSTM-Mean: 0.5575609764456749\n",
            "LSTM-STD: 0.030855344448556508\n",
            "LSTM-Best: 0.6341463327407837\n",
            "LSTM-LOSS\n",
            "LSTM-Mean: 0.690781815648079\n",
            "LSTM-STD: 0.010798006145251957\n",
            "LSTM-Best: 0.6761513352394104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Feature Reduction "
      ],
      "metadata": {
        "id": "cN-XHrt8Vo6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Traditional Machine Learning Models"
      ],
      "metadata": {
        "id": "kCHxIIGxfJtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### GridSearch the best parameters for SVM.SVC, RandomForest and Logistic Regression"
      ],
      "metadata": {
        "id": "4FNegvkLv_Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {\n",
        "    'svm' : {\n",
        "        'model' : svm.SVC(),\n",
        "        'params' : {\n",
        "            'C' : [0.5,1,5,10,25,50,100,500,1000],\n",
        "            'kernel' : ['rbf', 'linear', 'poly', 'sigmoid'],\n",
        "            'gamma' : [1, 0.1, 0.001, 0.0001, 'auto', 'scale'],\n",
        "            'degree' : [1, 2, 3, 4, 5, 6, 7]\n",
        "        }\n",
        "    },\n",
        "    'logistic_regression' : {\n",
        "        'model' : LogisticRegression(solver='liblinear', multi_class='auto'),\n",
        "        'params' : {\n",
        "            'C' : [0.5,1,5,10,25,50,100,500,1000],\n",
        "            'penalty' : ['l1', 'l2']\n",
        "        }\n",
        "    },\n",
        "    'random_forest' : {\n",
        "        'model' : RandomForestClassifier(),\n",
        "        'params' : {\n",
        "          'n_estimators' : [1,5,10,25,50,100,250,500,750,1000],\n",
        "          'criterion' : ['gini', 'entropy', 'log_loss'],\n",
        "          'max_features' : ['sqrt', 'log2', None]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "BtnL_OQUv-HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "\n",
        "sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10,random_state=42)\n",
        "#sssplit=RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=42)\n",
        "\n",
        "for model_name, mp in model_params.items():\n",
        "  clf = GridSearchCV(mp['model'], mp['params'], cv=sssplit, return_train_score=False,verbose=1)\n",
        "  clf.fit(trad_X,trad_y)\n",
        "  scores.append({\n",
        "      'model' : model_name,\n",
        "      'best_estimator' : clf.best_estimator_,\n",
        "      'best_score' : clf.best_score_,\n",
        "      'best_params' : clf.best_params_\n",
        "  })\n",
        "\n",
        "scores"
      ],
      "metadata": {
        "id": "9zCQuiJixumL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cross Validation\n",
        "\n",
        "###### Having discovered the best parameters for each model, now we perform cross-validation, using StratifiedShuffleSplit, to train and test our models. The results of this process will tell us which model achieves the best accuracy."
      ],
      "metadata": {
        "id": "N_-3Epmg14Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVCResults = []\n",
        "LogisticRegressionResults = []\n",
        "RandomForestsResults = []\n",
        "\n",
        "for i in range(10):\n",
        "  print(i)\n",
        "\n",
        "  #sssplit=RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=42)\n",
        "  sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10)\n",
        "\n",
        "  modelSVC = svm.SVC(C = scores[0]['best_params']['C'], gamma=scores[0]['best_params']['gamma'], kernel=scores[0]['best_params']['kernel'], degree=scores[0]['best_params']['degree'])\n",
        "  results=sklearn.model_selection.cross_val_score(modelSVC,trad_X,trad_y,cv=sssplit)\n",
        "  #print(\"SVC-Mean:\",np.mean(results))\n",
        "  #print(\"SVC-STD:\",np.std(results))\n",
        "  #print(\"SVC-Best:\",np.amax(results))\n",
        "  SVCResults.append(results)\n",
        "\n",
        "\n",
        "  modelLogisticRegression = LogisticRegression(solver='liblinear', multi_class='auto', C = scores[1]['best_params']['C'])\n",
        "  results=sklearn.model_selection.cross_val_score(modelLogisticRegression,trad_X,trad_y,cv=sssplit)\n",
        "  #print(\"LogisticRegression-Mean:\",np.mean(results))\n",
        "  #print(\"LogisticRegression-STD:\",np.std(results))\n",
        "  #print(\"LogisticRegression-Best:\",np.amax(results))\n",
        "  LogisticRegressionResults.append(results)\n",
        "\n",
        "  modelRandomForest = RandomForestClassifier(n_estimators=scores[2]['best_params']['n_estimators'])\n",
        "  results=sklearn.model_selection.cross_val_score(modelRandomForest,trad_X,trad_y,cv=sssplit)\n",
        "  #print(\"RandomForest-Mean:\",np.mean(results))\n",
        "  #print(\"RandomForest-STD:\",np.std(results))\n",
        "  #print(\"RandomForest-Best:\",np.amax(results))\n",
        "  RandomForestsResults.append(results)\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in SVCResults:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"SVC-Mean:\",np.mean(totals))\n",
        "print(\"SVC-STD:\",np.std(totals))\n",
        "print(\"SVC-Best:\",np.amax(totals))\n",
        "\n",
        "print()\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in LogisticRegressionResults:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"LogisticRegression-Mean:\",np.mean(totals))\n",
        "print(\"LogisticRegression-STD:\",np.std(totals))\n",
        "print(\"LogisticRegression-Best:\",np.amax(totals))\n",
        "\n",
        "print()\n",
        "\n",
        "totals = []\n",
        "\n",
        "for results in RandomForestsResults:\n",
        "  for result in results:\n",
        "    totals.append(result)\n",
        "\n",
        "print(\"RandomForests-Mean:\",np.mean(totals))\n",
        "print(\"RandomForests-STD:\",np.std(totals))\n",
        "print(\"RandomForests-Best:\",np.amax(totals))\n",
        "\n",
        "print()\n",
        "\n",
        "# Show how many times a certain model beat the others\n",
        "\n",
        "wins = {'SVC' : 0, 'LR' : 0, 'RF' : 0, 'TIE' : 0}\n",
        "\n",
        "for i in range(len(SVCResults)):\n",
        "  for j in range(len(SVCResults[0])):\n",
        "    if SVCResults[i][j] >= LogisticRegressionResults[i][j] and SVCResults[i][j] >= RandomForestsResults[i][j]:\n",
        "      wins['SVC']+=1\n",
        "\n",
        "    if LogisticRegressionResults[i][j] >= SVCResults[i][j] and LogisticRegressionResults[i][j] >= RandomForestsResults[i][j]:\n",
        "      wins['LR']+=1\n",
        "\n",
        "    if RandomForestsResults[i][j] >= LogisticRegressionResults[i][j] and RandomForestsResults[i][j] >= SVCResults[i][j]:\n",
        "      wins['RF']+=1\n",
        "    else:\n",
        "      wins['TIE']+=1\n",
        "print(\"single:\",wins)\n",
        "\n",
        "# Show how many times a certain model beat the others but for the means\n",
        "\n",
        "wins = {'SVC' : 0, 'LR' : 0, 'RF' : 0, 'TIE' : 0}\n",
        "\n",
        "for i in range(len(SVCResults)):\n",
        "  if np.mean(SVCResults[i]) >= np.mean(LogisticRegressionResults[i]) and np.mean(SVCResults[i]) >= np.mean(RandomForestsResults[i]):\n",
        "    wins['SVC']+=1\n",
        "\n",
        "  if np.mean(LogisticRegressionResults[i]) >= np.mean(SVCResults[i]) and np.mean(LogisticRegressionResults[i]) >= np.mean(RandomForestsResults[i]):\n",
        "    wins['LR']+=1\n",
        "\n",
        "  if np.mean(RandomForestsResults[i]) >= np.mean(LogisticRegressionResults[i]) and np.mean(RandomForestsResults[i]) >= np.mean(SVCResults[i]):\n",
        "    wins['RF']+=1\n",
        "\n",
        "  else:\n",
        "    wins['TIE']+=1\n",
        "print(\"mean:\",wins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1JZEfYjdXGP",
        "outputId": "f5d9c99e-31b1-41b2-e90d-079dadfc82f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "SVC-Mean: 0.7109756097560974\n",
            "SVC-STD: 0.06484090262467533\n",
            "SVC-Best: 0.8536585365853658\n",
            "\n",
            "LogisticRegression-Mean: 0.7182926829268291\n",
            "LogisticRegression-STD: 0.061255455389631036\n",
            "LogisticRegression-Best: 0.8292682926829268\n",
            "\n",
            "RandomForests-Mean: 0.7041463414634143\n",
            "RandomForests-STD: 0.06440640964847566\n",
            "RandomForests-Best: 0.8292682926829268\n",
            "\n",
            "single: {'SVC': 36, 'LR': 44, 'RF': 39, 'TIE': 61}\n",
            "mean: {'SVC': 5, 'LR': 4, 'RF': 2, 'TIE': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Features reduction\n",
        "\n",
        "###### First, we must discover which features have less impact, for each model's best estimator. \n",
        "\n",
        "######Then, remove them from the dataset and check the results of cross-validation again. \n",
        "\n",
        "######This process will ideally report the best results possible."
      ],
      "metadata": {
        "id": "kjV78LpA12IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVC\n",
        "# If the best kernel parameter for SVC was non-linear, then we cannot investigate feature importance\n",
        "# This is because non-linear kernels map the dataset into a space of higher dimension\n",
        "\n",
        "if(scores[0]['best_params']['kernel'] == 'linear'):\n",
        "  model = scores[0]['best_estimator']\n",
        "  prediction_test = model.predict(trad_X_test)\n",
        "  print(\"SVC-Accuracy:\",sklearn.metrics.accuracy_score(trad_y_test,prediction_test))\n",
        "\n",
        "  weightsSCVDF = pd.DataFrame(pd.Series(modelSVC.coef_[0], index=datasetDF.drop(columns=['Label']).columns).sort_values(ascending=False)).rename(columns = {0:'feature'})\n",
        "  weightsSCVDF.to_csv(rootFolder+\"/CSV_Data/FEATURES/weightsSCV.csv\", encoding='utf-8')\n",
        "\n",
        "  #sklearn.metrics.ConfusionMatrixDisplay.from_predictions(trad_y_test, prediction_test) #confusion matrix\n",
        "\n",
        "# Logistic Regression\n",
        "model = scores[1]['best_estimator']\n",
        "#model.fit(trad_X_train, trad_y_train)\n",
        "prediction_test = model.predict(trad_X_test)\n",
        "print(\"LogisticRegression-Accuracy:\",sklearn.metrics.accuracy_score(trad_y_test,prediction_test))\n",
        "\n",
        "weightsLogisticRegressionDF = pd.DataFrame(pd.Series(model.coef_[0], index=datasetDF.drop(columns=['Label']).columns).sort_values(ascending=False)).rename(columns = {0:'feature'})\n",
        "weightsLogisticRegressionDF.to_csv(rootFolder+\"/CSV_Data/FEATURES/weightsLogisticRegression.csv\", encoding='utf-8')\n",
        "\n",
        "#sklearn.metrics.ConfusionMatrixDisplay(\"Logistic Regression\").from_predictions(trad_y_test, prediction_test) #confusion matrix\n",
        "\n",
        "# Random Forest\n",
        "model = scores[2]['best_estimator']\n",
        "prediction_test = model.predict(trad_X_test)\n",
        "print(\"RandomForest-Accuracy:\",sklearn.metrics.accuracy_score(trad_y_test,prediction_test))\n",
        "\n",
        "weightsRandomForestDF = pd.Series(model.feature_importances_, index=datasetDF.drop(columns=['Label']).columns).sort_values(ascending=False).to_frame().rename(columns = {0:'feature'})\n",
        "weightsRandomForestDF.to_csv(rootFolder+\"/CSV_Data/FEATURES/weightsRandomForest.csv\", encoding='utf-8')\n",
        "\n",
        "#sklearn.metrics.ConfusionMatrixDisplay(\"Random Forest\").from_predictions(trad_y_test, prediction_test) #confusion matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoYnkAH2w3s3",
        "outputId": "d5b1a349-da36-4631-b9fb-6672e5cf6343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression-Accuracy: 0.7560975609756098\n",
            "RandomForest-Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We're going to try different thresholds for removing features\n",
        "\n",
        "SVCThresholds = [0.25,0.5,1.0,1.5,2.0,2.5,3.0]\n",
        "LRThresholds = [0.25,0.5,1.0,1.5,2.0,2.5,3.0]\n",
        "RFThresholds = [0.003,0.004,0.005,0.006,0.007,0.008,0.009]\n",
        "\n",
        "for k in range(len(SVCThresholds)):\n",
        "  print(\"iteration k:\",k)\n",
        "\n",
        "  ######################################################\n",
        "  # Remove the unnecessary features from the dataset\n",
        "  ######################################################\n",
        "\n",
        "  # SVC\n",
        "  if(scores[0]['best_params']['kernel'] == 'linear'):\n",
        "    datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/dataset.csv\")\n",
        "  \n",
        "    SVC_y = datasetDF['Label'].to_numpy()\n",
        "    SVC_X = datasetDF.drop(columns=['Label'])\n",
        "  \n",
        "    # Those that have feature importance below 1 and above -1 are probably safe to ignore\n",
        "    #weightsSCVDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/weightsSCV.csv\")\n",
        "    featuresToRemove = weightsSCVDF.loc[(weightsSCVDF['feature'] >= -SVCThresholds[k]) & (weightsSCVDF['feature'] <= SVCThresholds[k])].index.values.tolist()\n",
        "    SVC_X = SVC_X.drop(columns=featuresToRemove)\n",
        "  \n",
        "    SVC_X = tf.keras.utils.normalize(SVC_X, axis=1)  \n",
        "  \n",
        "  \n",
        "  # LogisticRegression\n",
        "  datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/dataset.csv\")\n",
        "  \n",
        "  LR_y = datasetDF['Label'].to_numpy()\n",
        "  LR_X = datasetDF.drop(columns=['Label'])\n",
        "  \n",
        "  # Those that have feature importance below 1 and above -1 are probably safe to ignore\n",
        "  #weightsLogisticRegressionDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/weightsLogisticRegression.csv\")\n",
        "  featuresToRemove = weightsLogisticRegressionDF.loc[(weightsLogisticRegressionDF['feature'] >= -LRThresholds[k]) & (weightsLogisticRegressionDF['feature'] <= LRThresholds[k])].index.values.tolist()\n",
        "  LR_X = LR_X.drop(columns=featuresToRemove)\n",
        "  \n",
        "  LR_X = tf.keras.utils.normalize(LR_X, axis=1)\n",
        "    \n",
        "  # RandomForest\n",
        "  datasetDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/dataset.csv\")\n",
        "  \n",
        "  RF_y = datasetDF['Label'].to_numpy()\n",
        "  RF_X = datasetDF.drop(columns=['Label'])\n",
        "  \n",
        "  # Those that have feature coeficients that are below 0.006 are probably safe to ignore\n",
        "  #weightsRandomForestDF = pd.read_csv(rootFolder+\"/CSV_Data/FEATURES/weightsRandomForest.csv\")\n",
        "  featuresToRemove = weightsRandomForestDF.loc[(weightsRandomForestDF['feature'] <= RFThresholds[k])].index.values.tolist()\n",
        "  RF_X = RF_X.drop(columns=featuresToRemove)\n",
        "  \n",
        "  RF_X = tf.keras.utils.normalize(RF_X, axis=1)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  ######################################################\n",
        "  # And check the results of cross-validation again\n",
        "  ######################################################\n",
        "  SVCResults = []\n",
        "  LogisticRegressionResults = []\n",
        "  RandomForestsResults = []\n",
        "\n",
        "  for i in range(10):\n",
        "    print(i)\n",
        "\n",
        "    #sssplit=RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=42)\n",
        "    sssplit = StratifiedShuffleSplit(n_splits=10,test_size=0.10)\n",
        "\n",
        "    if(scores[0]['best_params']['kernel'] == 'linear'):\n",
        "      modelSVC = svm.SVC(C = scores[0]['best_params']['C'], gamma=scores[0]['best_params']['gamma'], kernel=scores[0]['best_params']['kernel'], degree=scores[0]['best_params']['degree'])\n",
        "      results=sklearn.model_selection.cross_val_score(modelSVC,SVC_X,SVC_y,cv=sssplit)\n",
        "      #print(\"SVC-Mean:\",np.mean(results))\n",
        "      #print(\"SVC-STD:\",np.std(results))\n",
        "      #print(\"SVC-Best:\",np.amax(results))\n",
        "      SVCResults.append(results)\n",
        "\n",
        "\n",
        "    modelLogisticRegression = LogisticRegression(solver='liblinear', multi_class='auto', C = scores[1]['best_params']['C'])\n",
        "    results=sklearn.model_selection.cross_val_score(modelLogisticRegression,LR_X,LR_y,cv=sssplit)\n",
        "    #print(\"LogisticRegression-Mean:\",np.mean(results))\n",
        "    #print(\"LogisticRegression-STD:\",np.std(results))\n",
        "    #print(\"LogisticRegression-Best:\",np.amax(results))\n",
        "    LogisticRegressionResults.append(results)\n",
        "\n",
        "    modelRandomForest = RandomForestClassifier(n_estimators=scores[2]['best_params']['n_estimators'])\n",
        "    results=sklearn.model_selection.cross_val_score(modelRandomForest,RF_X,RF_y,cv=sssplit)\n",
        "    #print(\"RandomForest-Mean:\",np.mean(results))\n",
        "    #print(\"RandomForest-STD:\",np.std(results))\n",
        "    #print(\"RandomForest-Best:\",np.amax(results))\n",
        "    RandomForestsResults.append(results)\n",
        "\n",
        "\n",
        "\n",
        "  ######################################################\n",
        "  # Now check the scores\n",
        "  ######################################################\n",
        "  if(scores[0]['best_params']['kernel'] == 'linear'):\n",
        "    totals = []\n",
        "\n",
        "    for results in SVCResults:\n",
        "      for result in results:\n",
        "        totals.append(result)\n",
        "\n",
        "    print(\"SVC-Mean:\",np.mean(totals))\n",
        "    print(\"SVC-STD:\",np.std(totals))\n",
        "    print(\"SVC-Best:\",np.amax(totals))\n",
        "    print(\"SVC-Threshold\",SVCThresholds[k])\n",
        "\n",
        "    print()\n",
        "\n",
        "  totals = []\n",
        "\n",
        "  for results in LogisticRegressionResults:\n",
        "    for result in results:\n",
        "      totals.append(result)\n",
        "\n",
        "  print(\"LogisticRegression-Mean:\",np.mean(totals))\n",
        "  print(\"LogisticRegression-STD:\",np.std(totals))\n",
        "  print(\"LogisticRegression-Best:\",np.amax(totals))\n",
        "  print(\"LogisticRegression-Threshold\",LRThresholds[k])\n",
        "  \n",
        "\n",
        "  print()\n",
        "\n",
        "  totals = []\n",
        "\n",
        "  for results in RandomForestsResults:\n",
        "    for result in results:\n",
        "      totals.append(result)\n",
        "\n",
        "  print(\"RandomForests-Mean:\",np.mean(totals))\n",
        "  print(\"RandomForests-STD:\",np.std(totals))\n",
        "  print(\"RandomForests-Best:\",np.amax(totals))\n",
        "  print(\"RandomForests-Threshold\",RFThresholds[k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkeImJVf1ESa",
        "outputId": "dc0bde2d-c9d3-4d1b-ede4-f25701a68ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration k: 0\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.715121951219512\n",
            "LogisticRegression-STD: 0.06819505216507822\n",
            "LogisticRegression-Best: 0.8292682926829268\n",
            "LogisticRegression-Threshold 0.25\n",
            "\n",
            "RandomForests-Mean: 0.7214634146341463\n",
            "RandomForests-STD: 0.06698930455266522\n",
            "RandomForests-Best: 0.9024390243902439\n",
            "RandomForests-Threshold 0.003\n",
            "iteration k: 1\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.7202439024390245\n",
            "LogisticRegression-STD: 0.06077577008664903\n",
            "LogisticRegression-Best: 0.8292682926829268\n",
            "LogisticRegression-Threshold 0.5\n",
            "\n",
            "RandomForests-Mean: 0.7151219512195119\n",
            "RandomForests-STD: 0.05913112703487088\n",
            "RandomForests-Best: 0.8292682926829268\n",
            "RandomForests-Threshold 0.004\n",
            "iteration k: 2\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.7185365853658535\n",
            "LogisticRegression-STD: 0.06832925849827766\n",
            "LogisticRegression-Best: 0.8536585365853658\n",
            "LogisticRegression-Threshold 1.0\n",
            "\n",
            "RandomForests-Mean: 0.7282926829268292\n",
            "RandomForests-STD: 0.06256656181972685\n",
            "RandomForests-Best: 0.8780487804878049\n",
            "RandomForests-Threshold 0.005\n",
            "iteration k: 3\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.7202439024390244\n",
            "LogisticRegression-STD: 0.06420288706835509\n",
            "LogisticRegression-Best: 0.8780487804878049\n",
            "LogisticRegression-Threshold 1.5\n",
            "\n",
            "RandomForests-Mean: 0.7260975609756096\n",
            "RandomForests-STD: 0.05613519416369165\n",
            "RandomForests-Best: 0.8292682926829268\n",
            "RandomForests-Threshold 0.006\n",
            "iteration k: 4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.7158536585365853\n",
            "LogisticRegression-STD: 0.05796212018690802\n",
            "LogisticRegression-Best: 0.8780487804878049\n",
            "LogisticRegression-Threshold 2.0\n",
            "\n",
            "RandomForests-Mean: 0.7285365853658538\n",
            "RandomForests-STD: 0.06541452501764237\n",
            "RandomForests-Best: 0.926829268292683\n",
            "RandomForests-Threshold 0.007\n",
            "iteration k: 5\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.7180487804878046\n",
            "LogisticRegression-STD: 0.06682392714992326\n",
            "LogisticRegression-Best: 0.8536585365853658\n",
            "LogisticRegression-Threshold 2.5\n",
            "\n",
            "RandomForests-Mean: 0.7219512195121949\n",
            "RandomForests-STD: 0.06227875773074977\n",
            "RandomForests-Best: 0.8536585365853658\n",
            "RandomForests-Threshold 0.008\n",
            "iteration k: 6\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "LogisticRegression-Mean: 0.7190243902439023\n",
            "LogisticRegression-STD: 0.06630199686899468\n",
            "LogisticRegression-Best: 0.8780487804878049\n",
            "LogisticRegression-Threshold 3.0\n",
            "\n",
            "RandomForests-Mean: 0.7214634146341463\n",
            "RandomForests-STD: 0.0635252318866935\n",
            "RandomForests-Best: 0.8292682926829268\n",
            "RandomForests-Threshold 0.009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Results"
      ],
      "metadata": {
        "id": "nYUeYfdzP3Yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Traditional Machine Learning Models\n",
        "\n",
        "**Best parameters for each model were:**\n",
        "\n",
        "```\n",
        "SVC:\n",
        "* best_params: {'C': 50, 'degree': 1, 'gamma': 'scale', 'kernel': 'poly'}\n",
        "\n",
        "LogisticRegression:\n",
        "* best_params: {'C': 100}\n",
        "\n",
        "RandomForests:\n",
        "* best_params: {'n_estimators': 1000}\n",
        "```\n",
        "\n",
        "**Score for cross-validation were:**\n",
        "```\n",
        "SVC:\n",
        "* SVC-Mean: 0.7109756097560974\n",
        "* SVC-STD: 0.06484090262467533\n",
        "* SVC-Best: 0.8536585365853658\n",
        "\n",
        "LogisticRegression:\n",
        "* LogisticRegression-Mean: 0.7182926829268291\n",
        "* LogisticRegression-STD: 0.061255455389631036\n",
        "* LogisticRegression-Best: 0.8292682926829268\n",
        "\n",
        "RandomForests:\n",
        "* RandomForests-Mean: 0.7041463414634143\n",
        "* RandomForests-STD: 0.06440640964847566\n",
        "* RandomForests-Best: 0.8292682926829268\n",
        "\n",
        "Wins Count:\n",
        "* single: {'SVC': 36, 'LR': 44, 'RF': 39, 'TIE': 61}\n",
        "* mean: {'SVC': 5, 'LR': 4, 'RF': 2, 'TIE': 8}\n",
        "```\n",
        "\n",
        "**After feature reduction, the best results were:**\n",
        "```\n",
        "(None for SVC due to non-linear kernel mapping the features to higher dimension)\n",
        "\n",
        "LogisticRegression: (Nearly a 0.01 increase from before)\n",
        "* LogisticRegression-Mean: 0.7202439024390244\n",
        "* LogisticRegression-STD: 0.06420288706835509\n",
        "* LogisticRegression-Best: 0.8780487804878049\n",
        "* LogisticRegression-Threshold 1.5\n",
        "\n",
        "\n",
        "RandomForest (Nearly a 0.03 increase from before)\n",
        "* RandomForests-Mean: 0.7285365853658538\n",
        "* RandomForests-STD: 0.06541452501764237\n",
        "* RandomForests-Best: 0.926829268292683\n",
        "* RandomForests-Threshold 0.007\n",
        "```\n"
      ],
      "metadata": {
        "id": "PDPiab8NM-jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Learning Models\n",
        "\n",
        "**Best parameters for each model were:**\n",
        "\n",
        "```\n",
        "CNN:\n",
        "best_params: \n",
        "* {'learning_rate': ALWAYS 0.001\n",
        "  'convolutional_layers': ALWAYS 1,\n",
        "  'dropout': 0.05 (OR 1)\n",
        "  'kernels': 16 (OR 8)\n",
        "  'dense_layers': ALWAYS 1,\n",
        "  'dense_layer_kernels': ALWAYS 16} \n",
        "\n",
        "* 'epochs': ALWAYS 10\n",
        "* 'batch_size': 4 (OR 64)\n",
        "* 'early stop patience': 5 (for val_loss with val_split=0.1)\n",
        "\n",
        "LSTM:\n",
        "best_params: \n",
        "* {'learning_rate': ALWAYS 0.001\n",
        "  'lstm_layers': ALWAYS 1,\n",
        "  'units' : 32\n",
        "  'dropout': 0.05 (OR 0.1)\n",
        "  'dense_layers': ALWAYS 1,\n",
        "  'dense_layer_kernels': ALWAYS 16} \n",
        "\n",
        "* 'epochs': 20\n",
        "* 'batch_size': 4 (OR 64)\n",
        "* 'early stop patience': 5 (for val_loss with val_split=0.1)\n",
        "```\n",
        "\n",
        "**Score for cross-validation were:**\n",
        "```\n",
        "Accuracy of guessing just 0's 0.5722222222222222\n",
        "(given that the dataset was split with a stratified shuffle split)\n",
        "\n",
        "CNN:\n",
        "--ACCURACY\n",
        "* CNN-Mean: 0.6004878053069115\n",
        "* CNN-STD: 0.07058690043299648\n",
        "* CNN-Best: 0.7560975551605225\n",
        "-- LOSS\n",
        "* CNN-Mean: 0.7772195053100586\n",
        "* CNN-STD: 0.1319117559551849\n",
        "* CNN-Best: 0.5428411960601807\n",
        "\n",
        "\n",
        "LSTM:\n",
        "-- ACCURACY\n",
        "* LSTM-Mean: 0.5575609764456749\n",
        "* LSTM-STD: 0.030855344448556508\n",
        "* LSTM-Best: 0.6341463327407837\n",
        "--LOSS\n",
        "* LSTM-Mean: 0.690781815648079\n",
        "* LSTM-STD: 0.010798006145251957\n",
        "* LSTM-Best: 0.6761513352394104\n",
        "```\n",
        "\n",
        "**After feature reduction, the best results were:**\n",
        "```\n",
        "There was no feature reduction\n",
        "```\n"
      ],
      "metadata": {
        "id": "qmFB8YBkP-WP"
      }
    }
  ]
}